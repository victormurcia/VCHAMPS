{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNIeCLSVjTsqSdfY3OqRlfl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victormurcia/VCHAMPS/blob/main/Mapping_Encounter_IDs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuaMQLz53UCc",
        "outputId": "228184cc-2dc4-4bd6-b52f-3d7ea18ff891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#General utilities\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "import glob,shutil,os,warnings,math,time,sys,re\n",
        "from typing import List\n",
        "import dask.dataframe as dd\n",
        "from dask.diagnostics import ProgressBar\n",
        "\n",
        "#For converting states to their abbreviations\n",
        "#!pip install us\n",
        "#import us\n",
        "\n",
        "#For performing UTC normalization on datetime columns based on the STATE column\n",
        "import pytz\n",
        "\n",
        "#For Slider viz\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output,HTML\n",
        "\n",
        "#For EDA\n",
        "#!pip install dataprep\n",
        "#from dataprep.eda import create_report\n",
        "\n",
        "#Enable data to be extracted and downloaded from my Google Drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the path to the desired directory\n",
        "directory_path = r'/content/drive/MyDrive/VCHAMPS - Train Cleaned'\n",
        "\n",
        "# Change the current working directory to the desired directory\n",
        "os.chdir(directory_path)\n",
        "\n",
        "# Verify the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "print(f\"Current working directory: {cwd}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI6KlNP53VDJ",
        "outputId": "563e9e72-b75d-450a-d30a-81a47ce5e1db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/drive/MyDrive/VCHAMPS - Train Cleaned\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Encounter IDs\n",
        "Do this based on the inpatient_admisssions, ed_visits,and outpatient visits files.\n",
        "\n",
        "I'll either use a Hash or a UUID to define the Encounter ID"
      ],
      "metadata": {
        "id": "zB-qZRQ55iVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import uuid\n",
        "# Define a dictionary to store generated UUIDs\n",
        "uuid_dict = {}\n",
        "\n",
        "# Define a custom function to generate UUIDs and ensure uniqueness\n",
        "def generate_uuid(row, df_val):\n",
        "    if df_val == 1:\n",
        "      columns = ['Internalpatientid', 'Ed visit start date', 'Discharge date ed']\n",
        "    elif df_val == 2:\n",
        "      columns = ['Internalpatientid', 'Admission date', 'Discharge date']\n",
        "    elif df_val == 3:\n",
        "      columns = ['Internalpatientid', 'Visit start date']\n",
        "\n",
        "    unique_values = tuple(row[column] for column in columns)\n",
        "    unique_values += (df_val,)  # Append the additional parameter to the unique_values tuple\n",
        "    key = str(unique_values)\n",
        "\n",
        "    # Check if UUID already exists in the dictionary\n",
        "    if key in uuid_dict:\n",
        "        return uuid_dict[key]\n",
        "\n",
        "    # Generate a new UUID and store it in the dictionary\n",
        "    new_uuid = str(uuid.uuid5(uuid.NAMESPACE_OID, key))\n",
        "    uuid_dict[key] = new_uuid\n",
        "    return new_uuid"
      ],
      "metadata": {
        "id": "NkyD2Udw4IQw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib\n",
        "\n",
        "def generate_hash_id(row, df_val):\n",
        "    if df_val == 1:\n",
        "        columns = ['Internalpatientid', 'Ed visit start date', 'Discharge date ed']\n",
        "    elif df_val == 2:\n",
        "        columns = ['Internalpatientid', 'Admission date', 'Discharge date']\n",
        "    elif df_val == 3:\n",
        "        columns = ['Internalpatientid', 'Visit start date']\n",
        "\n",
        "    data = ''.join(str(row[column]) for column in columns)\n",
        "    data += str(df_val)\n",
        "\n",
        "    hash_object = hashlib.sha256(data.encode())\n",
        "    hash_id = hash_object.hexdigest()\n",
        "\n",
        "    return hash_id"
      ],
      "metadata": {
        "id": "m6zzM8sb5fU1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Dataframes\n",
        "ed_visits_df            = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/ed_visits.parquet/*.parquet')\n",
        "inpatient_admissions_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/inpatient_admissions.parquet')\n",
        "outpatient_visits_df    = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/outpatient_visits.parquet/*.parquet')"
      ],
      "metadata": {
        "id": "EThtBSjV6RAw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to use UUID instead of Hash since UUIDs used far less memory"
      ],
      "metadata": {
        "id": "M6NFd6J68nHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign UUIDs to each row\n",
        "ed_visits_df['Encounter ID'] = ed_visits_df.apply(generate_uuid, args=(1,), axis=1, meta=(None, 'object'))\n",
        "inpatient_admissions_df['Encounter ID'] = inpatient_admissions_df.apply(generate_uuid, args=(2,), axis=1, meta=(None, 'object'))\n",
        "outpatient_visits_df['Encounter ID'] = outpatient_visits_df.apply(generate_uuid, args=(3,), axis=1, meta=(None, 'object'))"
      ],
      "metadata": {
        "id": "1ZaP4VhI6Hwq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed_visits_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "87HdhYVr7Ufn",
        "outputId": "c55a1239-29ed-4b0e-8fc6-57bbd16db8db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-dcf49fe2c0b3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0med_visits_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# No need to warn if we're already looking at all partitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0msafe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnpartitions\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnpartitions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpartitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m_head\u001b[0;34m(self, n, npartitions, compute, safe)\u001b[0m\n\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1300\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0mpostcomputes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/threaded.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiprocessingPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     results = get_async(\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_workers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"waiting\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0mfire_tasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                         \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/dask/local.py\u001b[0m in \u001b[0;36mqueue_get\u001b[0;34m(q)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqueue_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_admissions_df.head()"
      ],
      "metadata": {
        "id": "cHK23iRe7Wi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outpatient_visits_df.head()"
      ],
      "metadata": {
        "id": "7TrMQAUv8Hkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that each of the visits have a UUID associated with it, I can start mapping the other dataframes to them"
      ],
      "metadata": {
        "id": "zPiogZaq8ubG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ed_visits_df = ed_visits_df.compute()\n",
        "inpatient_admissions_df = inpatient_admissions_df.compute()\n",
        "outpatient_visits_df = outpatient_visits_df.compute()"
      ],
      "metadata": {
        "id": "0CSlgFRsAwBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save them to parquet files\n",
        "# Save the Dask DataFrame as Parquet\n",
        "ed_visits_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/ed_visits.parquet', engine='pyarrow')\n",
        "inpatient_admissions_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/inpatient_admissions.parquet', engine='pyarrow')\n",
        "outpatient_visits_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/outpatient_visits.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "id": "q6Msg5VnoIb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ed_visits_df"
      ],
      "metadata": {
        "id": "cDxHnZ3zCptU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_admissions_df"
      ],
      "metadata": {
        "id": "EJfFCRu601qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outpatient_visits_df"
      ],
      "metadata": {
        "id": "InhhmxR04kg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a list of all use encounter IDs. Will be used to ensure uniqueness later\n",
        "encounter_ids = []\n",
        "encounter_ids.extend(ed_visits_df['Encounter ID'].tolist())\n",
        "encounter_ids.extend(inpatient_admissions_df['Encounter ID'].tolist())\n",
        "encounter_ids.extend(outpatient_visits_df['Encounter ID'].tolist())"
      ],
      "metadata": {
        "id": "unnpjj_E6WKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def map_encounter_id(row, age_col, date_col):\n",
        "    patient_id    = row['Internalpatientid']\n",
        "    patient_age   = row[age_col]\n",
        "    date_to_match = row[date_col]\n",
        "\n",
        "    #1. Check for matches in the ed_visits first\n",
        "    # Filter the first dataframe for matching patient ID and age conditions\n",
        "    filtered_df = ed_visits_df[(ed_visits_df['Internalpatientid'] == patient_id) & (ed_visits_df['Age at ed visit'] <= patient_age)]\n",
        "    # Find the first instance where date_to_match is between the start and end dates\n",
        "    filtered_df = filtered_df[(filtered_df['Ed visit start date'] <= date_to_match) & (filtered_df['Discharge date ed'] >= date_to_match)]\n",
        "    if len(filtered_df) > 0:\n",
        "        #print('Found match in ed_visits for ',patient_id)\n",
        "        return filtered_df['Encounter ID'].iloc[0]\n",
        "    else:\n",
        "        #2. Check for matches in the inpatient_visits if no match is found in ed_visits\n",
        "        filtered_df = inpatient_admissions_df[(inpatient_admissions_df['Internalpatientid'] == patient_id) & (inpatient_admissions_df['Age at admission'] <= patient_age)]\n",
        "         # Find the first instance where date_to_match is between the start and end dates\n",
        "        filtered_df = filtered_df[(filtered_df['Admission date'] <= date_to_match) & (filtered_df['Discharge date'] >= date_to_match)]\n",
        "        if len(filtered_df) > 0:\n",
        "          #print('Found match in inpatient_visits for ',patient_id)\n",
        "          return filtered_df['Encounter ID'].iloc[0]\n",
        "        else:\n",
        "          #3. Check for matches in the outpatient_visits if no match is found in inpatient_visits\n",
        "          filtered_df = outpatient_visits_df[(outpatient_visits_df['Internalpatientid'] == patient_id) & (outpatient_visits_df['Age at visit'] <= patient_age)]\n",
        "          # Find the first instance where date_to_match is between the start and end dates\n",
        "          filtered_df = filtered_df[(filtered_df['Visit start date'] <= date_to_match) & (filtered_df['Visit End Date'] >= date_to_match)]\n",
        "          if len(filtered_df) > 0:\n",
        "            #print('Found match in outpatient_visits for ',patient_id)\n",
        "            return filtered_df['Encounter ID'].iloc[0]\n",
        "          else:\n",
        "            #print('No match found. Producing unique Encounter ID')\n",
        "            return str(uuid.uuid4())\n",
        "    return None"
      ],
      "metadata": {
        "id": "bA3xD7u79zD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_death_df['Encounter ID'] = small_death_df.apply(lambda row: map_encounter_id(row, 'Age at death', 'Death date'), axis=1)"
      ],
      "metadata": {
        "id": "rByU7Fmeu83l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#I tested the function in a subset just to ensure proper operation and it works well and is fairly fast.\n",
        "small_death_df"
      ],
      "metadata": {
        "id": "RdjHQ27CvUAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping Death DF"
      ],
      "metadata": {
        "id": "YXFZjwaBOLIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "death_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/death.parquet')"
      ],
      "metadata": {
        "id": "J1cIWgiN8-df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "death_df = death_df.compute()"
      ],
      "metadata": {
        "id": "j2_3WMJov2Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "death_df['Encounter ID'] = death_df.apply(lambda row: map_encounter_id(row, 'Age at death', 'Death date'), axis=1)"
      ],
      "metadata": {
        "id": "O--2E-SgwLzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Dask DataFrame as Parquet\n",
        "death_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/death.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "id": "H27XrPTY-L-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping Inpatient Locations DF"
      ],
      "metadata": {
        "id": "CBZkz5rISfCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_location_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/inpatient_location.parquet')\n",
        "inpatient_location_df = inpatient_location_df.compute()\n",
        "inpatient_location_df.columns"
      ],
      "metadata": {
        "id": "lBwC3PtRN_vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_location_df['Encounter ID'] = inpatient_location_df.apply(lambda row: map_encounter_id(row, 'Age at location', 'Location start date'), axis=1)"
      ],
      "metadata": {
        "id": "WF6DdK8YQyqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_location_df"
      ],
      "metadata": {
        "id": "dBq1P382l_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Dask DataFrame as Parquet\n",
        "inpatient_location_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/inpatient_location.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "id": "3QKIkkAumPwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping Inpatient Specialty DF"
      ],
      "metadata": {
        "id": "jM1AA1JeSjnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_specialty_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/inpatient_specialty.parquet')\n",
        "inpatient_specialty_df = inpatient_specialty_df.compute()\n",
        "inpatient_specialty_df.columns"
      ],
      "metadata": {
        "id": "tDa1ZGs7RNfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inpatient_specialty_df['Encounter ID'] = inpatient_specialty_df.apply(lambda row: map_encounter_id(row, 'Age at specialty', 'Specialty start date'), axis=1)"
      ],
      "metadata": {
        "id": "WsFnt79zSxEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Dask DataFrame as Parquet\n",
        "inpatient_specialty_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/inpatient_specialty.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "id": "ReMHlkn4mnGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session Disconnected... Will continue from here..."
      ],
      "metadata": {
        "id": "3oi-Hnj3rMzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Dataframes\n",
        "ed_visits_df            = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/ed_visits.parquet')\n",
        "inpatient_admissions_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/inpatient_admissions.parquet')\n",
        "outpatient_visits_df    = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/outpatient_visits.parquet')\n",
        "\n",
        "ed_visits_df = ed_visits_df.compute()\n",
        "inpatient_admissions_df = inpatient_admissions_df.compute()\n",
        "outpatient_visits_df = outpatient_visits_df.compute()"
      ],
      "metadata": {
        "id": "ZbCODQcbrT7Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizing Mapping via Vectorization"
      ],
      "metadata": {
        "id": "rwrCkdhxUPXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_encounter_id_vectorized(row, age_col, date_col):\n",
        "    patient_id = row['Internalpatientid']\n",
        "    patient_age = row[age_col]\n",
        "    date_to_match = row[date_col]\n",
        "\n",
        "    filtered_ed_visits = ed_visits_df[ed_visits_df['Internalpatientid'] == patient_id]\n",
        "    ed_visit_match = (filtered_ed_visits['Ed visit start date'] <= date_to_match) & (filtered_ed_visits['Discharge date ed'] >= date_to_match) & (filtered_ed_visits['Age at ed visit'] <= patient_age)\n",
        "    if ed_visit_match.any():\n",
        "        return filtered_ed_visits.loc[ed_visit_match, 'Encounter ID'].iloc[0]\n",
        "\n",
        "    filtered_inpatient_admissions = inpatient_admissions_df[inpatient_admissions_df['Internalpatientid'] == patient_id]\n",
        "    inpatient_match = (filtered_inpatient_admissions['Admission date'] <= date_to_match) & (filtered_inpatient_admissions['Discharge date'] >= date_to_match) & (filtered_inpatient_admissions['Age at admission'] <= patient_age)\n",
        "    if inpatient_match.any():\n",
        "        return filtered_inpatient_admissions.loc[inpatient_match, 'Encounter ID'].iloc[0]\n",
        "\n",
        "    filtered_outpatient_visits = outpatient_visits_df[outpatient_visits_df['Internalpatientid'] == patient_id]\n",
        "    outpatient_match = (filtered_outpatient_visits['Visit start date'] <= date_to_match) & (filtered_outpatient_visits['Visit End Date'] >= date_to_match) & (filtered_outpatient_visits['Age at visit'] <= patient_age)\n",
        "    if outpatient_match.any():\n",
        "        return filtered_outpatient_visits.loc[outpatient_match, 'Encounter ID'].iloc[0]\n",
        "\n",
        "    return str(uuid.uuid4())"
      ],
      "metadata": {
        "id": "NHhk-FVQUVvK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping Measurements DF"
      ],
      "metadata": {
        "id": "6DzkCikgTAbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "measurements_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/measurements.parquet/*.parquet')\n",
        "measurements_df = measurements_df.compute()\n",
        "measurements_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gv75lRJNTNqQ",
        "outputId": "539d01e5-c4b7-4659-d3e5-cd145d00d2fa"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Internalpatientid', 'Age at measurement', 'Measurement date',\n",
              "       'Measurement', 'Result numeric'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "measurements_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Gf_-CA1VxhRR",
        "outputId": "f7fbc771-211c-4235-9eeb-68be6535c0f8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Internalpatientid  Age at measurement    Measurement date  \\\n",
              "0                       1                  59 2003-05-21 00:27:01   \n",
              "0                    9713                  68 2017-08-17 18:10:50   \n",
              "0                   97124                  72 2019-02-09 17:56:52   \n",
              "0                  107718                  63 2020-12-16 02:10:45   \n",
              "0                   96334                  72 2016-01-09 06:23:49   \n",
              "...                   ...                 ...                 ...   \n",
              "783886             101271                  66 2011-05-23 02:24:16   \n",
              "783887             101271                  66 2011-05-23 02:24:16   \n",
              "783889             101271                  67 2012-03-29 18:28:09   \n",
              "783890             101271                  67 2012-03-29 18:28:09   \n",
              "783891             101271                  67 2012-03-29 18:28:09   \n",
              "\n",
              "             Measurement  Result numeric  \n",
              "0            Temperature       95.804066  \n",
              "0         Pulse oximetry       88.000000  \n",
              "0                  Pulse       66.000000  \n",
              "0            Temperature       95.979629  \n",
              "0                 Height       68.793928  \n",
              "...                  ...             ...  \n",
              "783886              Pain        0.000000  \n",
              "783887            Weight      180.451035  \n",
              "783889  Respiratory rate       18.000000  \n",
              "783890              Pain        0.000000  \n",
              "783891    Pulse oximetry      103.000000  \n",
              "\n",
              "[102380786 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-eebb3f6a-b2c6-4759-a674-cc71f8cd0c4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Internalpatientid</th>\n",
              "      <th>Age at measurement</th>\n",
              "      <th>Measurement date</th>\n",
              "      <th>Measurement</th>\n",
              "      <th>Result numeric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>59</td>\n",
              "      <td>2003-05-21 00:27:01</td>\n",
              "      <td>Temperature</td>\n",
              "      <td>95.804066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9713</td>\n",
              "      <td>68</td>\n",
              "      <td>2017-08-17 18:10:50</td>\n",
              "      <td>Pulse oximetry</td>\n",
              "      <td>88.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>97124</td>\n",
              "      <td>72</td>\n",
              "      <td>2019-02-09 17:56:52</td>\n",
              "      <td>Pulse</td>\n",
              "      <td>66.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>107718</td>\n",
              "      <td>63</td>\n",
              "      <td>2020-12-16 02:10:45</td>\n",
              "      <td>Temperature</td>\n",
              "      <td>95.979629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>96334</td>\n",
              "      <td>72</td>\n",
              "      <td>2016-01-09 06:23:49</td>\n",
              "      <td>Height</td>\n",
              "      <td>68.793928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783886</th>\n",
              "      <td>101271</td>\n",
              "      <td>66</td>\n",
              "      <td>2011-05-23 02:24:16</td>\n",
              "      <td>Pain</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783887</th>\n",
              "      <td>101271</td>\n",
              "      <td>66</td>\n",
              "      <td>2011-05-23 02:24:16</td>\n",
              "      <td>Weight</td>\n",
              "      <td>180.451035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783889</th>\n",
              "      <td>101271</td>\n",
              "      <td>67</td>\n",
              "      <td>2012-03-29 18:28:09</td>\n",
              "      <td>Respiratory rate</td>\n",
              "      <td>18.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783890</th>\n",
              "      <td>101271</td>\n",
              "      <td>67</td>\n",
              "      <td>2012-03-29 18:28:09</td>\n",
              "      <td>Pain</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783891</th>\n",
              "      <td>101271</td>\n",
              "      <td>67</td>\n",
              "      <td>2012-03-29 18:28:09</td>\n",
              "      <td>Pulse oximetry</td>\n",
              "      <td>103.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>102380786 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eebb3f6a-b2c6-4759-a674-cc71f8cd0c4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-4b21b179-7ef9-4100-8477-f895e0594fd0\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4b21b179-7ef9-4100-8477-f895e0594fd0')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-4b21b179-7ef9-4100-8477-f895e0594fd0 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-eebb3f6a-b2c6-4759-a674-cc71f8cd0c4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-eebb3f6a-b2c6-4759-a674-cc71f8cd0c4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I'll sample 10M rows of the dataframe. This should take ~20 hours to map\n",
        "sampled_measurements_df = measurements_df.sample(n=10000000, random_state=42)\n",
        "sampled_measurements_df = sampled_measurements_df.reset_index(drop=True)\n",
        "sampled_measurements_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8T_CMUqRvKdW",
        "outputId": "e726535a-4490-4176-ae79-0d7c75551de3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Internalpatientid  Age at measurement    Measurement date  \\\n",
              "0                   127057                  63 2011-10-09 22:49:08   \n",
              "1                   160816                  95 2005-09-22 21:00:15   \n",
              "2                   120458                  77 2008-08-19 11:23:04   \n",
              "3                   142658                  66 2006-11-26 11:12:16   \n",
              "4                    34927                  64 2011-11-12 00:12:58   \n",
              "...                    ...                 ...                 ...   \n",
              "9999995              78496                  86 2016-05-15 11:16:54   \n",
              "9999996             122619                  58 2007-12-24 18:18:57   \n",
              "9999997             107225                  82 2002-08-14 17:44:17   \n",
              "9999998               2727                  54 2008-09-12 08:23:02   \n",
              "9999999             164692                  80 2008-12-24 01:11:56   \n",
              "\n",
              "              Measurement  Result numeric  \n",
              "0                   Pulse       75.000000  \n",
              "1                   Pulse       66.000000  \n",
              "2             Temperature       97.282646  \n",
              "3          Pulse oximetry       98.000000  \n",
              "4                   Pulse       58.000000  \n",
              "...                   ...             ...  \n",
              "9999995             Pulse       71.000000  \n",
              "9999996  Respiratory rate       18.000000  \n",
              "9999997       Temperature       95.925387  \n",
              "9999998              Pain        3.000000  \n",
              "9999999       Temperature       93.630568  \n",
              "\n",
              "[10000000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-be816998-d310-4036-b918-a9ca7dad57aa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Internalpatientid</th>\n",
              "      <th>Age at measurement</th>\n",
              "      <th>Measurement date</th>\n",
              "      <th>Measurement</th>\n",
              "      <th>Result numeric</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>127057</td>\n",
              "      <td>63</td>\n",
              "      <td>2011-10-09 22:49:08</td>\n",
              "      <td>Pulse</td>\n",
              "      <td>75.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>160816</td>\n",
              "      <td>95</td>\n",
              "      <td>2005-09-22 21:00:15</td>\n",
              "      <td>Pulse</td>\n",
              "      <td>66.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>120458</td>\n",
              "      <td>77</td>\n",
              "      <td>2008-08-19 11:23:04</td>\n",
              "      <td>Temperature</td>\n",
              "      <td>97.282646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>142658</td>\n",
              "      <td>66</td>\n",
              "      <td>2006-11-26 11:12:16</td>\n",
              "      <td>Pulse oximetry</td>\n",
              "      <td>98.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34927</td>\n",
              "      <td>64</td>\n",
              "      <td>2011-11-12 00:12:58</td>\n",
              "      <td>Pulse</td>\n",
              "      <td>58.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999995</th>\n",
              "      <td>78496</td>\n",
              "      <td>86</td>\n",
              "      <td>2016-05-15 11:16:54</td>\n",
              "      <td>Pulse</td>\n",
              "      <td>71.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999996</th>\n",
              "      <td>122619</td>\n",
              "      <td>58</td>\n",
              "      <td>2007-12-24 18:18:57</td>\n",
              "      <td>Respiratory rate</td>\n",
              "      <td>18.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999997</th>\n",
              "      <td>107225</td>\n",
              "      <td>82</td>\n",
              "      <td>2002-08-14 17:44:17</td>\n",
              "      <td>Temperature</td>\n",
              "      <td>95.925387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999998</th>\n",
              "      <td>2727</td>\n",
              "      <td>54</td>\n",
              "      <td>2008-09-12 08:23:02</td>\n",
              "      <td>Pain</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999999</th>\n",
              "      <td>164692</td>\n",
              "      <td>80</td>\n",
              "      <td>2008-12-24 01:11:56</td>\n",
              "      <td>Temperature</td>\n",
              "      <td>93.630568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000000 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be816998-d310-4036-b918-a9ca7dad57aa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-19defdea-1bf2-4917-a076-70f337db4ece\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-19defdea-1bf2-4917-a076-70f337db4ece')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-19defdea-1bf2-4917-a076-70f337db4ece button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be816998-d310-4036-b918-a9ca7dad57aa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be816998-d310-4036-b918-a9ca7dad57aa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/measurements'\n",
        "# Define the chunk size\n",
        "chunk_size = 100000\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = math.ceil(len(measurements_df) / chunk_size)\n",
        "\n",
        "# Create an empty list to store the encounter IDs\n",
        "encounter_ids = []\n",
        "\n",
        "# Iterate over chunks\n",
        "for i in range(num_chunks):\n",
        "    start_idx = i * chunk_size\n",
        "    end_idx = (i + 1) * chunk_size\n",
        "\n",
        "    # Get the chunk of dataframe\n",
        "    chunk_df = measurements_df[start_idx:end_idx]\n",
        "\n",
        "    # Process the chunk and track progress using tqdm\n",
        "    for _, row in tqdm(chunk_df.iterrows(), total=chunk_df.shape[0], desc=f\"Processing Chunk {i+1}/{num_chunks}\"):\n",
        "        encounter_id = map_encounter_id_vectorized(row, 'Age at measurement', 'Measurement date')\n",
        "        encounter_ids.append(encounter_id)\n",
        "\n",
        "    # Create a new DataFrame with the chunk results\n",
        "    chunk_results_df = chunk_df.copy()\n",
        "    chunk_results_df['Encounter ID'] = encounter_ids[start_idx:end_idx]\n",
        "\n",
        "    # Save the results of the chunk to Parquet file\n",
        "    chunk_results_df.to_parquet(f'{save_path}/measurements{i+1}.parquet', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jVcPH473EOj",
        "outputId": "c2de0780-c1db-4f15-a05b-d0c8ee732bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Chunk 1/1024: 100%|██████████| 100000/100000 [19:04<00:00, 87.34it/s]\n",
            "Processing Chunk 2/1024: 100%|██████████| 100000/100000 [18:44<00:00, 88.96it/s]\n",
            "Processing Chunk 3/1024: 100%|██████████| 100000/100000 [18:48<00:00, 88.60it/s]\n",
            "Processing Chunk 4/1024: 100%|██████████| 100000/100000 [19:51<00:00, 83.90it/s]\n",
            "Processing Chunk 5/1024: 100%|██████████| 100000/100000 [19:23<00:00, 85.94it/s]\n",
            "Processing Chunk 6/1024: 100%|██████████| 100000/100000 [19:09<00:00, 86.98it/s]\n",
            "Processing Chunk 7/1024: 100%|██████████| 100000/100000 [19:12<00:00, 86.79it/s]\n",
            "Processing Chunk 8/1024: 100%|██████████| 100000/100000 [18:54<00:00, 88.15it/s]\n",
            "Processing Chunk 9/1024: 100%|██████████| 100000/100000 [19:18<00:00, 86.31it/s]\n",
            "Processing Chunk 10/1024: 100%|██████████| 100000/100000 [18:37<00:00, 89.49it/s]\n",
            "Processing Chunk 11/1024: 100%|██████████| 100000/100000 [19:26<00:00, 85.70it/s]\n",
            "Processing Chunk 12/1024: 100%|██████████| 100000/100000 [18:55<00:00, 88.07it/s]\n",
            "Processing Chunk 13/1024: 100%|██████████| 100000/100000 [19:24<00:00, 85.85it/s]\n",
            "Processing Chunk 14/1024: 100%|██████████| 100000/100000 [18:57<00:00, 87.94it/s]\n",
            "Processing Chunk 15/1024: 100%|██████████| 100000/100000 [19:04<00:00, 87.39it/s]\n",
            "Processing Chunk 16/1024: 100%|██████████| 100000/100000 [18:50<00:00, 88.49it/s]\n",
            "Processing Chunk 17/1024: 100%|██████████| 100000/100000 [19:18<00:00, 86.31it/s]\n",
            "Processing Chunk 18/1024: 100%|██████████| 100000/100000 [19:25<00:00, 85.77it/s]\n",
            "Processing Chunk 19/1024: 100%|██████████| 100000/100000 [19:13<00:00, 86.68it/s]\n",
            "Processing Chunk 20/1024: 100%|██████████| 100000/100000 [19:27<00:00, 85.63it/s]\n",
            "Processing Chunk 21/1024: 100%|██████████| 100000/100000 [19:12<00:00, 86.79it/s]\n",
            "Processing Chunk 22/1024: 100%|██████████| 100000/100000 [18:57<00:00, 87.88it/s]\n",
            "Processing Chunk 23/1024: 100%|██████████| 100000/100000 [19:19<00:00, 86.21it/s]\n",
            "Processing Chunk 24/1024: 100%|██████████| 100000/100000 [19:19<00:00, 86.28it/s]\n",
            "Processing Chunk 25/1024: 100%|██████████| 100000/100000 [19:11<00:00, 86.86it/s]\n",
            "Processing Chunk 26/1024: 100%|██████████| 100000/100000 [18:25<00:00, 90.42it/s]\n",
            "Processing Chunk 27/1024: 100%|██████████| 100000/100000 [19:43<00:00, 84.50it/s]\n",
            "Processing Chunk 28/1024: 100%|██████████| 100000/100000 [19:26<00:00, 85.74it/s]\n",
            "Processing Chunk 29/1024: 100%|██████████| 100000/100000 [19:30<00:00, 85.45it/s]\n",
            "Processing Chunk 30/1024: 100%|██████████| 100000/100000 [19:10<00:00, 86.93it/s]\n",
            "Processing Chunk 31/1024: 100%|██████████| 100000/100000 [19:29<00:00, 85.47it/s]\n",
            "Processing Chunk 32/1024: 100%|██████████| 100000/100000 [19:08<00:00, 87.06it/s]\n",
            "Processing Chunk 33/1024: 100%|██████████| 100000/100000 [19:22<00:00, 86.04it/s]\n",
            "Processing Chunk 34/1024: 100%|██████████| 100000/100000 [18:59<00:00, 87.72it/s]\n",
            "Processing Chunk 35/1024: 100%|██████████| 100000/100000 [19:33<00:00, 85.21it/s]\n",
            "Processing Chunk 36/1024: 100%|██████████| 100000/100000 [19:12<00:00, 86.76it/s]\n",
            "Processing Chunk 37/1024: 100%|██████████| 100000/100000 [19:14<00:00, 86.65it/s]\n",
            "Processing Chunk 38/1024: 100%|██████████| 100000/100000 [19:27<00:00, 85.65it/s]\n",
            "Processing Chunk 39/1024: 100%|██████████| 100000/100000 [19:29<00:00, 85.52it/s]\n",
            "Processing Chunk 40/1024: 100%|██████████| 100000/100000 [19:19<00:00, 86.25it/s]\n",
            "Processing Chunk 41/1024: 100%|██████████| 100000/100000 [19:45<00:00, 84.33it/s]\n",
            "Processing Chunk 42/1024: 100%|██████████| 100000/100000 [19:35<00:00, 85.07it/s]\n",
            "Processing Chunk 43/1024: 100%|██████████| 100000/100000 [19:30<00:00, 85.46it/s]\n",
            "Processing Chunk 44/1024: 100%|██████████| 100000/100000 [19:38<00:00, 84.87it/s]\n",
            "Processing Chunk 45/1024: 100%|██████████| 100000/100000 [19:44<00:00, 84.45it/s]\n",
            "Processing Chunk 46/1024: 100%|██████████| 100000/100000 [19:39<00:00, 84.77it/s]\n",
            "Processing Chunk 47/1024: 100%|██████████| 100000/100000 [19:25<00:00, 85.81it/s]\n",
            "Processing Chunk 48/1024: 100%|██████████| 100000/100000 [19:15<00:00, 86.56it/s]\n",
            "Processing Chunk 49/1024: 100%|██████████| 100000/100000 [19:53<00:00, 83.82it/s]\n",
            "Processing Chunk 50/1024: 100%|██████████| 100000/100000 [19:37<00:00, 84.95it/s]\n",
            "Processing Chunk 51/1024: 100%|██████████| 100000/100000 [19:35<00:00, 85.05it/s]\n",
            "Processing Chunk 52/1024: 100%|██████████| 100000/100000 [19:44<00:00, 84.43it/s]\n",
            "Processing Chunk 53/1024: 100%|██████████| 100000/100000 [19:42<00:00, 84.54it/s]\n",
            "Processing Chunk 54/1024: 100%|██████████| 100000/100000 [19:27<00:00, 85.63it/s]\n",
            "Processing Chunk 55/1024: 100%|██████████| 100000/100000 [19:54<00:00, 83.70it/s]\n",
            "Processing Chunk 56/1024: 100%|██████████| 100000/100000 [19:54<00:00, 83.71it/s]\n",
            "Processing Chunk 57/1024: 100%|██████████| 100000/100000 [20:14<00:00, 82.36it/s]\n",
            "Processing Chunk 58/1024: 100%|██████████| 100000/100000 [20:00<00:00, 83.29it/s]\n",
            "Processing Chunk 59/1024: 100%|██████████| 100000/100000 [19:50<00:00, 83.98it/s]\n",
            "Processing Chunk 60/1024: 100%|██████████| 100000/100000 [19:33<00:00, 85.18it/s]\n",
            "Processing Chunk 61/1024: 100%|██████████| 100000/100000 [20:12<00:00, 82.48it/s]\n",
            "Processing Chunk 62/1024: 100%|██████████| 100000/100000 [20:26<00:00, 81.55it/s]\n",
            "Processing Chunk 63/1024: 100%|██████████| 100000/100000 [19:55<00:00, 83.68it/s]\n",
            "Processing Chunk 64/1024: 100%|██████████| 100000/100000 [20:04<00:00, 83.00it/s]\n",
            "Processing Chunk 65/1024: 100%|██████████| 100000/100000 [19:42<00:00, 84.58it/s]\n",
            "Processing Chunk 66/1024: 100%|██████████| 100000/100000 [20:02<00:00, 83.14it/s]\n",
            "Processing Chunk 67/1024: 100%|██████████| 100000/100000 [19:56<00:00, 83.55it/s]\n",
            "Processing Chunk 68/1024: 100%|██████████| 100000/100000 [20:00<00:00, 83.30it/s]\n",
            "Processing Chunk 69/1024: 100%|██████████| 100000/100000 [19:33<00:00, 85.24it/s]\n",
            "Processing Chunk 70/1024: 100%|██████████| 100000/100000 [19:43<00:00, 84.51it/s]\n",
            "Processing Chunk 71/1024: 100%|██████████| 100000/100000 [19:19<00:00, 86.24it/s]\n",
            "Processing Chunk 72/1024: 100%|██████████| 100000/100000 [19:28<00:00, 85.56it/s]\n",
            "Processing Chunk 73/1024: 100%|██████████| 100000/100000 [19:38<00:00, 84.88it/s] \n",
            "Processing Chunk 74/1024:  86%|████████▌ | 85919/100000 [17:05<02:46, 84.53it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmmm let me pivot the table first"
      ],
      "metadata": {
        "id": "oWeSJ9z4FcGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/measurements'\n",
        "# Define the chunk size\n",
        "chunk_size = 100000\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = math.ceil(len(measurements_df) / chunk_size)\n",
        "\n",
        "# Create an empty list to store the encounter IDs\n",
        "encounter_ids = []\n",
        "# Iterate over chunks starting from chunk 60\n",
        "for i in range(73, num_chunks):\n",
        "    start_idx = i * chunk_size\n",
        "    end_idx = (i + 1) * chunk_size\n",
        "\n",
        "    # Get the chunk of dataframe\n",
        "    chunk_df = measurements_df[start_idx:end_idx]\n",
        "\n",
        "    # Create an empty list to store the encounter IDs for the current chunk\n",
        "    chunk_encounter_ids = []\n",
        "\n",
        "    # Process the chunk and track progress using tqdm\n",
        "    for _, row in tqdm(chunk_df.iterrows(), total=chunk_df.shape[0], desc=f\"Processing Chunk {i+1}/{num_chunks}\"):\n",
        "        encounter_id = map_encounter_id_vectorized(row, 'Age at measurement', 'Measurement date')\n",
        "        chunk_encounter_ids.append(encounter_id)\n",
        "\n",
        "    # Create a new DataFrame with the chunk results\n",
        "    chunk_results_df = chunk_df.copy()\n",
        "    chunk_results_df['Encounter ID'] = chunk_encounter_ids\n",
        "\n",
        "    # Save the results of the chunk to Parquet file\n",
        "    chunk_results_df.to_parquet(f'{save_path}/measurements{i+1}.parquet', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTfa_wHlKHzN",
        "outputId": "8031c558-6213-48ce-c38a-f7f87f52f2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Chunk 74/1024: 100%|██████████| 100000/100000 [21:32<00:00, 77.38it/s]\n",
            "Processing Chunk 75/1024: 100%|██████████| 100000/100000 [21:46<00:00, 76.53it/s]\n",
            "Processing Chunk 76/1024: 100%|██████████| 100000/100000 [21:45<00:00, 76.59it/s]\n",
            "Processing Chunk 77/1024: 100%|██████████| 100000/100000 [21:19<00:00, 78.13it/s]\n",
            "Processing Chunk 78/1024: 100%|██████████| 100000/100000 [21:19<00:00, 78.16it/s]\n",
            "Processing Chunk 79/1024: 100%|██████████| 100000/100000 [21:20<00:00, 78.12it/s]\n",
            "Processing Chunk 80/1024: 100%|██████████| 100000/100000 [21:05<00:00, 79.05it/s]\n",
            "Processing Chunk 81/1024: 100%|██████████| 100000/100000 [20:52<00:00, 79.81it/s]\n",
            "Processing Chunk 82/1024: 100%|██████████| 100000/100000 [21:42<00:00, 76.75it/s]\n",
            "Processing Chunk 83/1024: 100%|██████████| 100000/100000 [21:11<00:00, 78.62it/s]\n",
            "Processing Chunk 84/1024: 100%|██████████| 100000/100000 [20:57<00:00, 79.53it/s]\n",
            "Processing Chunk 85/1024: 100%|██████████| 100000/100000 [21:28<00:00, 77.61it/s]\n",
            "Processing Chunk 86/1024: 100%|██████████| 100000/100000 [20:51<00:00, 79.91it/s]\n",
            "Processing Chunk 87/1024: 100%|██████████| 100000/100000 [21:24<00:00, 77.85it/s]\n",
            "Processing Chunk 88/1024: 100%|██████████| 100000/100000 [21:16<00:00, 78.34it/s]\n",
            "Processing Chunk 89/1024: 100%|██████████| 100000/100000 [21:26<00:00, 77.73it/s]\n",
            "Processing Chunk 90/1024: 100%|██████████| 100000/100000 [21:16<00:00, 78.34it/s]\n",
            "Processing Chunk 91/1024: 100%|██████████| 100000/100000 [22:01<00:00, 75.67it/s]\n",
            "Processing Chunk 92/1024: 100%|██████████| 100000/100000 [21:03<00:00, 79.12it/s]\n",
            "Processing Chunk 93/1024: 100%|██████████| 100000/100000 [20:53<00:00, 79.79it/s]\n",
            "Processing Chunk 94/1024: 100%|██████████| 100000/100000 [20:46<00:00, 80.19it/s]\n",
            "Processing Chunk 95/1024: 100%|██████████| 100000/100000 [20:38<00:00, 80.77it/s]\n",
            "Processing Chunk 96/1024: 100%|██████████| 100000/100000 [20:44<00:00, 80.37it/s]\n",
            "Processing Chunk 97/1024: 100%|██████████| 100000/100000 [20:31<00:00, 81.20it/s]\n",
            "Processing Chunk 98/1024: 100%|██████████| 100000/100000 [20:56<00:00, 79.61it/s]\n",
            "Processing Chunk 99/1024: 100%|██████████| 100000/100000 [21:20<00:00, 78.07it/s]\n",
            "Processing Chunk 100/1024: 100%|██████████| 100000/100000 [20:50<00:00, 79.96it/s]\n",
            "Processing Chunk 101/1024: 100%|██████████| 100000/100000 [20:54<00:00, 79.72it/s]\n",
            "Processing Chunk 102/1024: 100%|██████████| 100000/100000 [21:10<00:00, 78.72it/s]\n",
            "Processing Chunk 103/1024: 100%|██████████| 100000/100000 [20:37<00:00, 80.83it/s]\n",
            "Processing Chunk 104/1024: 100%|██████████| 100000/100000 [20:57<00:00, 79.49it/s]\n",
            "Processing Chunk 105/1024: 100%|██████████| 100000/100000 [21:32<00:00, 77.37it/s]\n",
            "Processing Chunk 106/1024: 100%|██████████| 100000/100000 [20:59<00:00, 79.37it/s]\n",
            "Processing Chunk 107/1024: 100%|██████████| 100000/100000 [20:58<00:00, 79.47it/s]\n",
            "Processing Chunk 108/1024: 100%|██████████| 100000/100000 [20:35<00:00, 80.96it/s]\n",
            "Processing Chunk 109/1024: 100%|██████████| 100000/100000 [20:20<00:00, 81.93it/s]\n",
            "Processing Chunk 110/1024: 100%|██████████| 100000/100000 [20:39<00:00, 80.70it/s]\n",
            "Processing Chunk 111/1024: 100%|██████████| 100000/100000 [21:03<00:00, 79.14it/s]\n",
            "Processing Chunk 112/1024: 100%|██████████| 100000/100000 [20:55<00:00, 79.67it/s]\n",
            "Processing Chunk 113/1024: 100%|██████████| 100000/100000 [20:49<00:00, 80.02it/s]\n",
            "Processing Chunk 114/1024: 100%|██████████| 100000/100000 [21:32<00:00, 77.38it/s] \n",
            "Processing Chunk 115/1024: 100%|██████████| 100000/100000 [20:55<00:00, 79.65it/s]\n",
            "Processing Chunk 116/1024: 100%|██████████| 100000/100000 [21:24<00:00, 77.88it/s]\n",
            "Processing Chunk 117/1024: 100%|██████████| 100000/100000 [21:23<00:00, 77.91it/s]\n",
            "Processing Chunk 118/1024: 100%|██████████| 100000/100000 [21:01<00:00, 79.26it/s]\n",
            "Processing Chunk 119/1024: 100%|██████████| 100000/100000 [21:16<00:00, 78.35it/s]\n",
            "Processing Chunk 120/1024: 100%|██████████| 100000/100000 [21:27<00:00, 77.64it/s]\n",
            "Processing Chunk 121/1024: 100%|██████████| 100000/100000 [22:10<00:00, 75.15it/s]\n",
            "Processing Chunk 122/1024: 100%|██████████| 100000/100000 [21:25<00:00, 77.81it/s]\n",
            "Processing Chunk 123/1024: 100%|██████████| 100000/100000 [21:10<00:00, 78.69it/s]\n",
            "Processing Chunk 124/1024: 100%|██████████| 100000/100000 [21:59<00:00, 75.81it/s]\n",
            "Processing Chunk 125/1024: 100%|██████████| 100000/100000 [21:54<00:00, 76.09it/s]\n",
            "Processing Chunk 126/1024: 100%|██████████| 100000/100000 [21:12<00:00, 78.61it/s]\n",
            "Processing Chunk 127/1024: 100%|██████████| 100000/100000 [21:48<00:00, 76.43it/s]\n",
            "Processing Chunk 128/1024: 100%|██████████| 100000/100000 [21:13<00:00, 78.49it/s]\n",
            "Processing Chunk 129/1024: 100%|██████████| 100000/100000 [21:09<00:00, 78.78it/s]\n",
            "Processing Chunk 130/1024: 100%|██████████| 100000/100000 [21:13<00:00, 78.52it/s]\n",
            "Processing Chunk 131/1024: 100%|██████████| 100000/100000 [21:08<00:00, 78.85it/s]\n",
            "Processing Chunk 132/1024: 100%|██████████| 100000/100000 [21:48<00:00, 76.44it/s]\n",
            "Processing Chunk 133/1024: 100%|██████████| 100000/100000 [21:19<00:00, 78.18it/s]\n",
            "Processing Chunk 134/1024: 100%|██████████| 100000/100000 [21:31<00:00, 77.40it/s]\n",
            "Processing Chunk 135/1024: 100%|██████████| 100000/100000 [21:42<00:00, 76.79it/s]\n",
            "Processing Chunk 136/1024: 100%|██████████| 100000/100000 [21:12<00:00, 78.59it/s]\n",
            "Processing Chunk 137/1024: 100%|██████████| 100000/100000 [21:35<00:00, 77.21it/s]\n",
            "Processing Chunk 138/1024: 100%|██████████| 100000/100000 [21:04<00:00, 79.07it/s]\n",
            "Processing Chunk 139/1024: 100%|██████████| 100000/100000 [21:12<00:00, 78.59it/s]\n",
            "Processing Chunk 140/1024: 100%|██████████| 100000/100000 [20:43<00:00, 80.42it/s]\n",
            "Processing Chunk 141/1024:  74%|███████▍  | 74022/100000 [15:59<05:16, 82.08it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/measurements'\n",
        "# Define the chunk size\n",
        "chunk_size = 100000\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = math.ceil(len(measurements_df) / chunk_size)\n",
        "\n",
        "# Create an empty list to store the encounter IDs\n",
        "encounter_ids = []\n",
        "# Iterate over chunks starting from chunk 60\n",
        "for i in range(140, num_chunks):\n",
        "    start_idx = i * chunk_size\n",
        "    end_idx = (i + 1) * chunk_size\n",
        "\n",
        "    # Get the chunk of dataframe\n",
        "    chunk_df = measurements_df[start_idx:end_idx]\n",
        "\n",
        "    # Create an empty list to store the encounter IDs for the current chunk\n",
        "    chunk_encounter_ids = []\n",
        "\n",
        "    # Process the chunk and track progress using tqdm\n",
        "    for _, row in tqdm(chunk_df.iterrows(), total=chunk_df.shape[0], desc=f\"Processing Chunk {i+1}/{num_chunks}\"):\n",
        "        encounter_id = map_encounter_id_vectorized(row, 'Age at measurement', 'Measurement date')\n",
        "        chunk_encounter_ids.append(encounter_id)\n",
        "\n",
        "    # Create a new DataFrame with the chunk results\n",
        "    chunk_results_df = chunk_df.copy()\n",
        "    chunk_results_df['Encounter ID'] = chunk_encounter_ids\n",
        "\n",
        "    # Save the results of the chunk to Parquet file\n",
        "    chunk_results_df.to_parquet(f'{save_path}/measurements{i+1}.parquet', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq7oWujsKa00",
        "outputId": "2aa5dd6c-323e-490b-f11d-6e59f89c360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Chunk 141/1024: 100%|██████████| 100000/100000 [24:03<00:00, 69.30it/s]\n",
            "Processing Chunk 142/1024: 100%|██████████| 100000/100000 [23:30<00:00, 70.90it/s]\n",
            "Processing Chunk 143/1024: 100%|██████████| 100000/100000 [24:01<00:00, 69.38it/s]\n",
            "Processing Chunk 144/1024: 100%|██████████| 100000/100000 [24:29<00:00, 68.06it/s]\n",
            "Processing Chunk 145/1024: 100%|██████████| 100000/100000 [23:15<00:00, 71.65it/s]\n",
            "Processing Chunk 146/1024: 100%|██████████| 100000/100000 [23:13<00:00, 71.79it/s]\n",
            "Processing Chunk 147/1024: 100%|██████████| 100000/100000 [23:53<00:00, 69.78it/s]\n",
            "Processing Chunk 148/1024: 100%|██████████| 100000/100000 [23:43<00:00, 70.23it/s]\n",
            "Processing Chunk 149/1024: 100%|██████████| 100000/100000 [23:35<00:00, 70.64it/s]\n",
            "Processing Chunk 150/1024: 100%|██████████| 100000/100000 [23:32<00:00, 70.82it/s]\n",
            "Processing Chunk 151/1024: 100%|██████████| 100000/100000 [23:50<00:00, 69.92it/s]\n",
            "Processing Chunk 152/1024: 100%|██████████| 100000/100000 [23:45<00:00, 70.13it/s]\n",
            "Processing Chunk 153/1024: 100%|██████████| 100000/100000 [23:34<00:00, 70.70it/s]\n",
            "Processing Chunk 154/1024: 100%|██████████| 100000/100000 [22:54<00:00, 72.73it/s]\n",
            "Processing Chunk 155/1024: 100%|██████████| 100000/100000 [23:48<00:00, 70.01it/s]\n",
            "Processing Chunk 156/1024: 100%|██████████| 100000/100000 [24:33<00:00, 67.87it/s]\n",
            "Processing Chunk 157/1024: 100%|██████████| 100000/100000 [24:15<00:00, 68.70it/s]\n",
            "Processing Chunk 158/1024: 100%|██████████| 100000/100000 [23:11<00:00, 71.88it/s]\n",
            "Processing Chunk 159/1024: 100%|██████████| 100000/100000 [24:05<00:00, 69.20it/s]\n",
            "Processing Chunk 160/1024: 100%|██████████| 100000/100000 [24:52<00:00, 67.02it/s]\n",
            "Processing Chunk 161/1024: 100%|██████████| 100000/100000 [22:50<00:00, 72.96it/s]\n",
            "Processing Chunk 162/1024: 100%|██████████| 100000/100000 [24:00<00:00, 69.40it/s]\n",
            "Processing Chunk 163/1024: 100%|██████████| 100000/100000 [23:49<00:00, 69.93it/s]\n",
            "Processing Chunk 164/1024: 100%|██████████| 100000/100000 [24:18<00:00, 68.56it/s]\n",
            "Processing Chunk 165/1024: 100%|██████████| 100000/100000 [24:42<00:00, 67.44it/s]\n",
            "Processing Chunk 166/1024: 100%|██████████| 100000/100000 [23:55<00:00, 69.65it/s]\n",
            "Processing Chunk 167/1024: 100%|██████████| 100000/100000 [23:44<00:00, 70.20it/s]\n",
            "Processing Chunk 168/1024: 100%|██████████| 100000/100000 [24:09<00:00, 68.98it/s]\n",
            "Processing Chunk 169/1024: 100%|██████████| 100000/100000 [23:26<00:00, 71.10it/s]\n",
            "Processing Chunk 170/1024: 100%|██████████| 100000/100000 [23:17<00:00, 71.57it/s]\n",
            "Processing Chunk 171/1024: 100%|██████████| 100000/100000 [24:20<00:00, 68.48it/s]\n",
            "Processing Chunk 172/1024: 100%|██████████| 100000/100000 [24:17<00:00, 68.61it/s]\n",
            "Processing Chunk 173/1024: 100%|██████████| 100000/100000 [24:03<00:00, 69.25it/s]\n",
            "Processing Chunk 174/1024: 100%|██████████| 100000/100000 [24:11<00:00, 68.91it/s]\n",
            "Processing Chunk 175/1024: 100%|██████████| 100000/100000 [23:59<00:00, 69.46it/s]\n",
            "Processing Chunk 176/1024: 100%|██████████| 100000/100000 [23:30<00:00, 70.92it/s]\n",
            "Processing Chunk 177/1024: 100%|██████████| 100000/100000 [24:12<00:00, 68.83it/s]\n",
            "Processing Chunk 178/1024: 100%|██████████| 100000/100000 [24:03<00:00, 69.30it/s]\n",
            "Processing Chunk 179/1024: 100%|██████████| 100000/100000 [25:09<00:00, 66.25it/s]\n",
            "Processing Chunk 180/1024: 100%|██████████| 100000/100000 [24:04<00:00, 69.23it/s]\n",
            "Processing Chunk 181/1024: 100%|██████████| 100000/100000 [23:12<00:00, 71.83it/s]\n",
            "Processing Chunk 182/1024: 100%|██████████| 100000/100000 [23:36<00:00, 70.58it/s]\n",
            "Processing Chunk 183/1024: 100%|██████████| 100000/100000 [22:56<00:00, 72.66it/s]\n",
            "Processing Chunk 184/1024: 100%|██████████| 100000/100000 [22:53<00:00, 72.80it/s]\n",
            "Processing Chunk 185/1024: 100%|██████████| 100000/100000 [23:27<00:00, 71.04it/s]\n",
            "Processing Chunk 186/1024: 100%|██████████| 100000/100000 [23:07<00:00, 72.05it/s]\n",
            "Processing Chunk 187/1024: 100%|██████████| 100000/100000 [23:21<00:00, 71.34it/s]\n",
            "Processing Chunk 188/1024: 100%|██████████| 100000/100000 [23:59<00:00, 69.49it/s]\n",
            "Processing Chunk 189/1024: 100%|██████████| 100000/100000 [24:51<00:00, 67.04it/s]\n",
            "Processing Chunk 190/1024: 100%|██████████| 100000/100000 [24:37<00:00, 67.68it/s]\n",
            "Processing Chunk 191/1024: 100%|██████████| 100000/100000 [24:31<00:00, 67.94it/s]\n",
            "Processing Chunk 192/1024: 100%|██████████| 100000/100000 [24:21<00:00, 68.44it/s]\n",
            "Processing Chunk 193/1024: 100%|██████████| 100000/100000 [24:27<00:00, 68.16it/s]\n",
            "Processing Chunk 194/1024: 100%|██████████| 100000/100000 [24:30<00:00, 68.02it/s]\n",
            "Processing Chunk 195/1024: 100%|██████████| 100000/100000 [24:44<00:00, 67.37it/s]\n",
            "Processing Chunk 196/1024: 100%|██████████| 100000/100000 [24:25<00:00, 68.23it/s]\n",
            "Processing Chunk 197/1024: 100%|██████████| 100000/100000 [23:53<00:00, 69.77it/s]\n",
            "Processing Chunk 198/1024: 100%|██████████| 100000/100000 [23:42<00:00, 70.30it/s]\n",
            "Processing Chunk 199/1024: 100%|██████████| 100000/100000 [23:30<00:00, 70.89it/s]\n",
            "Processing Chunk 200/1024: 100%|██████████| 100000/100000 [24:24<00:00, 68.27it/s]\n",
            "Processing Chunk 201/1024:  14%|█▍        | 14429/100000 [03:23<17:32, 81.33it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/measurements'\n",
        "# Define the chunk size\n",
        "chunk_size = 100000\n",
        "\n",
        "# Calculate the number of chunks\n",
        "num_chunks = math.ceil(len(measurements_df) / chunk_size)\n",
        "\n",
        "# Create an empty list to store the encounter IDs\n",
        "encounter_ids = []\n",
        "# Iterate over chunks starting from chunk 60\n",
        "for i in range(200, num_chunks):\n",
        "    start_idx = i * chunk_size\n",
        "    end_idx = (i + 1) * chunk_size\n",
        "\n",
        "    # Get the chunk of dataframe\n",
        "    chunk_df = measurements_df[start_idx:end_idx]\n",
        "\n",
        "    # Create an empty list to store the encounter IDs for the current chunk\n",
        "    chunk_encounter_ids = []\n",
        "\n",
        "    # Process the chunk and track progress using tqdm\n",
        "    for _, row in tqdm(chunk_df.iterrows(), total=chunk_df.shape[0], desc=f\"Processing Chunk {i+1}/{num_chunks}\"):\n",
        "        encounter_id = map_encounter_id_vectorized(row, 'Age at measurement', 'Measurement date')\n",
        "        chunk_encounter_ids.append(encounter_id)\n",
        "\n",
        "    # Create a new DataFrame with the chunk results\n",
        "    chunk_results_df = chunk_df.copy()\n",
        "    chunk_results_df['Encounter ID'] = chunk_encounter_ids\n",
        "\n",
        "    # Save the results of the chunk to Parquet file\n",
        "    chunk_results_df.to_parquet(f'{save_path}/measurements{i+1}.parquet', index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b4szDnpbTwC",
        "outputId": "1ddae894-a7d0-4399-d276-596bcc3cf3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Chunk 201/1024: 100%|██████████| 100000/100000 [19:57<00:00, 83.53it/s]\n",
            "Processing Chunk 202/1024: 100%|██████████| 100000/100000 [19:45<00:00, 84.33it/s]\n",
            "Processing Chunk 203/1024: 100%|██████████| 100000/100000 [19:32<00:00, 85.29it/s]\n",
            "Processing Chunk 204/1024: 100%|██████████| 100000/100000 [19:38<00:00, 84.86it/s]\n",
            "Processing Chunk 205/1024: 100%|██████████| 100000/100000 [19:16<00:00, 86.48it/s]\n",
            "Processing Chunk 206/1024: 100%|██████████| 100000/100000 [19:11<00:00, 86.88it/s]\n",
            "Processing Chunk 207/1024: 100%|██████████| 100000/100000 [19:24<00:00, 85.90it/s]\n",
            "Processing Chunk 208/1024: 100%|██████████| 100000/100000 [19:41<00:00, 84.62it/s]\n",
            "Processing Chunk 209/1024: 100%|██████████| 100000/100000 [19:28<00:00, 85.55it/s]\n",
            "Processing Chunk 210/1024: 100%|██████████| 100000/100000 [19:13<00:00, 86.66it/s]\n",
            "Processing Chunk 211/1024: 100%|██████████| 100000/100000 [19:54<00:00, 83.70it/s]\n",
            "Processing Chunk 212/1024: 100%|██████████| 100000/100000 [20:10<00:00, 82.61it/s]\n",
            "Processing Chunk 213/1024: 100%|██████████| 100000/100000 [20:08<00:00, 82.73it/s]\n",
            "Processing Chunk 214/1024: 100%|██████████| 100000/100000 [20:36<00:00, 80.84it/s]\n",
            "Processing Chunk 215/1024: 100%|██████████| 100000/100000 [20:09<00:00, 82.71it/s]\n",
            "Processing Chunk 216/1024: 100%|██████████| 100000/100000 [19:27<00:00, 85.67it/s]\n",
            "Processing Chunk 217/1024: 100%|██████████| 100000/100000 [20:06<00:00, 82.85it/s]\n",
            "Processing Chunk 218/1024: 100%|██████████| 100000/100000 [19:36<00:00, 84.99it/s]\n",
            "Processing Chunk 219/1024: 100%|██████████| 100000/100000 [20:15<00:00, 82.30it/s]\n",
            "Processing Chunk 220/1024: 100%|██████████| 100000/100000 [19:56<00:00, 83.60it/s]\n",
            "Processing Chunk 221/1024: 100%|██████████| 100000/100000 [19:32<00:00, 85.28it/s]\n",
            "Processing Chunk 222/1024: 100%|██████████| 100000/100000 [19:55<00:00, 83.66it/s]\n",
            "Processing Chunk 223/1024: 100%|██████████| 100000/100000 [19:52<00:00, 83.88it/s]\n",
            "Processing Chunk 224/1024: 100%|██████████| 100000/100000 [19:57<00:00, 83.49it/s]\n",
            "Processing Chunk 225/1024: 100%|██████████| 100000/100000 [19:52<00:00, 83.89it/s]\n",
            "Processing Chunk 226/1024: 100%|██████████| 100000/100000 [20:15<00:00, 82.27it/s]\n",
            "Processing Chunk 227/1024: 100%|██████████| 100000/100000 [19:43<00:00, 84.53it/s]\n",
            "Processing Chunk 228/1024: 100%|██████████| 100000/100000 [20:28<00:00, 81.37it/s]\n",
            "Processing Chunk 229/1024: 100%|██████████| 100000/100000 [19:51<00:00, 83.95it/s]\n",
            "Processing Chunk 230/1024: 100%|██████████| 100000/100000 [20:20<00:00, 81.90it/s]\n",
            "Processing Chunk 231/1024: 100%|██████████| 100000/100000 [19:45<00:00, 84.37it/s]\n",
            "Processing Chunk 232/1024: 100%|██████████| 100000/100000 [20:01<00:00, 83.22it/s]\n",
            "Processing Chunk 233/1024: 100%|██████████| 100000/100000 [20:35<00:00, 80.94it/s]\n",
            "Processing Chunk 234/1024: 100%|██████████| 100000/100000 [19:37<00:00, 84.92it/s]\n",
            "Processing Chunk 235/1024: 100%|██████████| 100000/100000 [19:35<00:00, 85.06it/s]\n",
            "Processing Chunk 236/1024: 100%|██████████| 100000/100000 [19:38<00:00, 84.82it/s]\n",
            "Processing Chunk 237/1024: 100%|██████████| 100000/100000 [19:34<00:00, 85.11it/s]\n",
            "Processing Chunk 238/1024: 100%|██████████| 100000/100000 [19:29<00:00, 85.52it/s]\n",
            "Processing Chunk 239/1024: 100%|██████████| 100000/100000 [19:29<00:00, 85.49it/s]\n",
            "Processing Chunk 240/1024: 100%|██████████| 100000/100000 [20:04<00:00, 83.03it/s]\n",
            "Processing Chunk 241/1024: 100%|██████████| 100000/100000 [20:31<00:00, 81.22it/s]\n",
            "Processing Chunk 242/1024: 100%|██████████| 100000/100000 [20:02<00:00, 83.14it/s]\n",
            "Processing Chunk 243/1024: 100%|██████████| 100000/100000 [19:48<00:00, 84.12it/s]\n",
            "Processing Chunk 244/1024: 100%|██████████| 100000/100000 [19:58<00:00, 83.44it/s]\n",
            "Processing Chunk 245/1024: 100%|██████████| 100000/100000 [20:14<00:00, 82.31it/s]\n",
            "Processing Chunk 246/1024: 100%|██████████| 100000/100000 [20:02<00:00, 83.16it/s]\n",
            "Processing Chunk 247/1024: 100%|██████████| 100000/100000 [20:03<00:00, 83.11it/s]\n",
            "Processing Chunk 248/1024: 100%|██████████| 100000/100000 [19:44<00:00, 84.46it/s]\n",
            "Processing Chunk 249/1024: 100%|██████████| 100000/100000 [19:57<00:00, 83.52it/s]\n",
            "Processing Chunk 250/1024: 100%|██████████| 100000/100000 [19:16<00:00, 86.50it/s]\n",
            "Processing Chunk 251/1024: 100%|██████████| 100000/100000 [19:56<00:00, 83.57it/s]\n",
            "Processing Chunk 252/1024: 100%|██████████| 100000/100000 [19:12<00:00, 86.75it/s]\n",
            "Processing Chunk 253/1024: 100%|██████████| 100000/100000 [19:48<00:00, 84.15it/s]\n",
            "Processing Chunk 254/1024: 100%|██████████| 100000/100000 [19:48<00:00, 84.11it/s]\n",
            "Processing Chunk 255/1024: 100%|██████████| 100000/100000 [19:31<00:00, 85.33it/s]\n",
            "Processing Chunk 256/1024: 100%|██████████| 100000/100000 [19:56<00:00, 83.56it/s]\n",
            "Processing Chunk 257/1024: 100%|██████████| 100000/100000 [20:00<00:00, 83.27it/s]\n",
            "Processing Chunk 258/1024: 100%|██████████| 100000/100000 [19:21<00:00, 86.13it/s]\n",
            "Processing Chunk 259/1024: 100%|██████████| 100000/100000 [19:21<00:00, 86.12it/s]\n",
            "Processing Chunk 260/1024: 100%|██████████| 100000/100000 [19:24<00:00, 85.86it/s]\n",
            "Processing Chunk 261/1024: 100%|██████████| 100000/100000 [19:55<00:00, 83.63it/s]\n",
            "Processing Chunk 262/1024: 100%|██████████| 100000/100000 [20:09<00:00, 82.71it/s]\n",
            "Processing Chunk 263/1024: 100%|██████████| 100000/100000 [20:14<00:00, 82.31it/s]\n",
            "Processing Chunk 264/1024: 100%|██████████| 100000/100000 [20:04<00:00, 83.02it/s]\n",
            "Processing Chunk 265/1024: 100%|██████████| 100000/100000 [20:02<00:00, 83.16it/s]\n",
            "Processing Chunk 266/1024: 100%|██████████| 100000/100000 [19:39<00:00, 84.79it/s]\n",
            "Processing Chunk 267/1024: 100%|██████████| 100000/100000 [20:41<00:00, 80.54it/s]\n",
            "Processing Chunk 268/1024: 100%|██████████| 100000/100000 [20:22<00:00, 81.80it/s]\n",
            "Processing Chunk 269/1024: 100%|██████████| 100000/100000 [20:30<00:00, 81.26it/s]\n",
            "Processing Chunk 270/1024: 100%|██████████| 100000/100000 [20:53<00:00, 79.76it/s]\n",
            "Processing Chunk 271/1024: 100%|██████████| 100000/100000 [20:45<00:00, 80.29it/s]\n",
            "Processing Chunk 272/1024: 100%|██████████| 100000/100000 [20:22<00:00, 81.77it/s]\n",
            "Processing Chunk 273/1024:  31%|███       | 31159/100000 [06:07<14:35, 78.65it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#measurements_df['Encounter ID'] = measurements_df.apply(lambda row: map_encounter_id(row, 'Age at measurement', 'Measurement date'), axis=1)"
      ],
      "metadata": {
        "id": "ATZ1J6YoTl4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Dask DataFrame as Parquet\n",
        "measurements_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/measurements.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "id": "3DtA2KUbA1cG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
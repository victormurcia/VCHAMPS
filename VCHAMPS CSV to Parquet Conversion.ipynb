{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641b5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm  # Import tqdm for the progress bar\n",
    "import glob,shutil,os,warnings,math\n",
    "from typing import List\n",
    "import dask.dataframe as dd\n",
    "\n",
    "#For Slider viz\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output,HTML\n",
    "\n",
    "#DataPrep for Quick EDA\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "#For ML model stuff\n",
    "from sklearn.datasets import load_iris,load_breast_cancer\n",
    "from sklearn.model_selection import learning_curve, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, roc_auc_score, confusion_matrix, log_loss\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4e163",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "The data was provided to us in .csv files. Some of these files are rather large however (file size > 30GB). This will be an issue with pandas since we won't have enough memory to load or do any operations on this data. I could try limiting the number of rows we load initially or load the data in chunks. However, I think it'll be more advantageous to do the following:\n",
    "* Convert the .csv files into .parquet files\n",
    "* Use dask instead of pandas whenever possible\n",
    "* Convert as many attributes that have dtype=object into something else whenever possible. For instance, the dates included  in the dataset, should be converted to datetime objects. \n",
    "* Convert as many numerical data from int64 to the lowest possible int type that does not compromise precision\n",
    "\n",
    "These improvements should not only make much more efficient usage of memory, but it should ultimately result in an overall optimization of speed for all subsequent queries.\n",
    "\n",
    "The cell below contains functions to create pandas dataframes from the original .csv files. I'll do that just to create comparisons between the processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e5d6b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\VCHAMPS Data\\Quality Check\n",
      "['D:\\\\VCHAMPS Data\\\\Quality Check\\\\conditions_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\demographics_event_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\demographics_static_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\ed_visits_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\immunization_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\inpatient_admissions_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\inpatient_location_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\inpatient_specialty_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\lab_results_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\measurements_blood_pressure_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\measurements_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\medications_administered_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\medications_ordered_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\outpatient_visits_qual.csv', 'D:\\\\VCHAMPS Data\\\\Quality Check\\\\procedures_qual.csv']\n"
     ]
    }
   ],
   "source": [
    "def load_csvs(path2data: str) -> List[str]:\n",
    "  \"\"\"\n",
    "  Load and return a list of CSV file paths from the specified directory.\n",
    "\n",
    "  Args:\n",
    "      path2data (str): The directory path containing the CSV files.\n",
    "\n",
    "  Returns:\n",
    "      List[str]: A list of CSV file paths.\n",
    "\n",
    "  \"\"\"\n",
    "  csv_files = glob.glob(path2data + '/*.csv')\n",
    "  return csv_files\n",
    "\n",
    "def make_df_list(csv_files: List[str]) -> List[pd.DataFrame]:\n",
    "  \"\"\"\n",
    "  Read CSV files from the provided list of file paths and return a list of DataFrames.\n",
    "\n",
    "  Args:\n",
    "      csv_files (List[str]): A list of CSV file paths.\n",
    "\n",
    "  Returns:\n",
    "      List[pd.DataFrame]: A list of DataFrames read from the CSV files.\n",
    "\n",
    "  \"\"\"\n",
    "  df_list = []\n",
    "  # Read the CSV file\n",
    "  for csv in csv_files:\n",
    "    df = pd.read_csv(csv)\n",
    "    df_list.append(df)\n",
    "\n",
    "  return df_list\n",
    "\n",
    "def clean_filenames(csv_files: List[str]) -> List[str]:\n",
    "  \"\"\"\n",
    "  Clean the file names by removing directory path and the .csv extension.\n",
    "\n",
    "  Args:\n",
    "      csv_files (List[str]): A list of CSV file paths.\n",
    "\n",
    "  Returns:\n",
    "      List[str]: A list of cleaned file names without directory path and file extension.\n",
    "\n",
    "  \"\"\"\n",
    "  #Get list of file names without directory junk and remove .csv extension from name\n",
    "  file_names = []\n",
    "\n",
    "  for file_path in csv_files:\n",
    "      file_name = os.path.basename(file_path)  # Get the file name with extension\n",
    "      file_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
    "      file_names.append(file_name)\n",
    "  return file_names\n",
    "\n",
    "# Specify the path to the desired directory\n",
    "directory_path = r'D:\\VCHAMPS Data\\Quality Check'\n",
    "\n",
    "# Change the current working directory to the desired directory\n",
    "os.chdir(directory_path)\n",
    "\n",
    "# Verify the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "#Define data location\n",
    "path2data = r'D:\\VCHAMPS Data\\Quality Check'\n",
    "#Load the .csv files into memory\n",
    "csv_files  = load_csvs(path2data)\n",
    "print(csv_files)\n",
    "#Create list of dataframes from csvs\n",
    "df_list_csv    = make_df_list(csv_files)\n",
    "#Clean the names of .csv files\n",
    "csv_file_names = clean_filenames(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da584e",
   "metadata": {},
   "source": [
    "The next cell will show how to convert a .csv file into a .parquet file using dask. I also compare the file size difference between a .csv and .parquet file. The differences are striking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "43a0c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV to Parquet conversion completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>CSV Size [MB]</th>\n",
       "      <th>Parquet Size [MB]</th>\n",
       "      <th>%Reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conditions_qual</td>\n",
       "      <td>72.757038</td>\n",
       "      <td>13.019831</td>\n",
       "      <td>82.105057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>demographics_event_qual</td>\n",
       "      <td>0.065379</td>\n",
       "      <td>0.037816</td>\n",
       "      <td>42.158851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demographics_static_qual</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.01626</td>\n",
       "      <td>69.320738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ed_visits_qual</td>\n",
       "      <td>0.40361</td>\n",
       "      <td>0.144735</td>\n",
       "      <td>64.139825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>immunization_qual</td>\n",
       "      <td>1.707275</td>\n",
       "      <td>0.43032</td>\n",
       "      <td>74.79494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>inpatient_admissions</td>\n",
       "      <td>1.047332</td>\n",
       "      <td>0.259045</td>\n",
       "      <td>75.266229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>inpatient_location_qual</td>\n",
       "      <td>0.712869</td>\n",
       "      <td>0.338958</td>\n",
       "      <td>52.451582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inpatient_specialty_qual</td>\n",
       "      <td>1.157339</td>\n",
       "      <td>0.493114</td>\n",
       "      <td>57.392477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lab_results_qual</td>\n",
       "      <td>249.419017</td>\n",
       "      <td>48.029166</td>\n",
       "      <td>80.743583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>measurements_blood_pressure_qual</td>\n",
       "      <td>14.068669</td>\n",
       "      <td>5.488777</td>\n",
       "      <td>60.985812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>measurements_qual</td>\n",
       "      <td>85.961599</td>\n",
       "      <td>18.533636</td>\n",
       "      <td>78.439633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>medications_administered_qual</td>\n",
       "      <td>136.444701</td>\n",
       "      <td>33.710924</td>\n",
       "      <td>75.293343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>medications_ordered_qual</td>\n",
       "      <td>75.903338</td>\n",
       "      <td>19.878344</td>\n",
       "      <td>73.810976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>outpatient_visits_qual</td>\n",
       "      <td>73.620619</td>\n",
       "      <td>15.662921</td>\n",
       "      <td>78.724818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>procedures_qual</td>\n",
       "      <td>169.322317</td>\n",
       "      <td>17.495858</td>\n",
       "      <td>89.667128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Filename CSV Size [MB] Parquet Size [MB]  \\\n",
       "0                    conditions_qual     72.757038         13.019831   \n",
       "1            demographics_event_qual      0.065379          0.037816   \n",
       "2           demographics_static_qual         0.053           0.01626   \n",
       "3                     ed_visits_qual       0.40361          0.144735   \n",
       "4                  immunization_qual      1.707275           0.43032   \n",
       "5               inpatient_admissions      1.047332          0.259045   \n",
       "6            inpatient_location_qual      0.712869          0.338958   \n",
       "7           inpatient_specialty_qual      1.157339          0.493114   \n",
       "8                   lab_results_qual    249.419017         48.029166   \n",
       "9   measurements_blood_pressure_qual     14.068669          5.488777   \n",
       "10                 measurements_qual     85.961599         18.533636   \n",
       "11     medications_administered_qual    136.444701         33.710924   \n",
       "12          medications_ordered_qual     75.903338         19.878344   \n",
       "13            outpatient_visits_qual     73.620619         15.662921   \n",
       "14                   procedures_qual    169.322317         17.495858   \n",
       "\n",
       "   %Reduction  \n",
       "0   82.105057  \n",
       "1   42.158851  \n",
       "2   69.320738  \n",
       "3   64.139825  \n",
       "4    74.79494  \n",
       "5   75.266229  \n",
       "6   52.451582  \n",
       "7   57.392477  \n",
       "8   80.743583  \n",
       "9   60.985812  \n",
       "10  78.439633  \n",
       "11  75.293343  \n",
       "12  73.810976  \n",
       "13  78.724818  \n",
       "14  89.667128  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert_csv_to_parquet(current_directory):\n",
    "    # Create an empty DataFrame to store the file sizes\n",
    "    file_sizes = pd.DataFrame(columns=['Filename', 'CSV Size', 'Parquet Size'])\n",
    "\n",
    "    # Find all CSV files in the input directory\n",
    "    csv_files = [file for file in os.listdir(current_directory) if file.endswith('.csv')]\n",
    "\n",
    "    # Process each CSV file\n",
    "    for i, csv_file in enumerate(csv_files):\n",
    "        # Create the file paths\n",
    "        input_file_path = os.path.join(current_directory, csv_file)\n",
    "        output_file_name = f\"{csv_file.replace('.csv', '')}.parquet\"\n",
    "        output_file_path = os.path.join(current_directory, output_file_name)\n",
    "\n",
    "        # Get the size of the CSV file\n",
    "        csv_file_size = os.path.getsize(input_file_path)\n",
    "\n",
    "        # Append the file size to the DataFrame\n",
    "        file_sizes = pd.concat([file_sizes, pd.DataFrame({\n",
    "            'Filename': [csv_file.replace('.csv', '')],\n",
    "            'CSV Size': [csv_file_size],\n",
    "            'Parquet Size': [None]  # Initialize with None\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        # Read the CSV file as a Dask DataFrame\n",
    "        df = dd.read_csv(input_file_path, dtype={\n",
    "            'Administered elsewhere': 'object',\n",
    "            'Dose unit': 'object',\n",
    "            'Result textual': 'object',\n",
    "            'Administration end date': 'object',\n",
    "            'Agentorangeflag': 'object',\n",
    "            'Combatflag': 'object',\n",
    "            'Ionizingradiationflag': 'object',\n",
    "            'Swasiaconditionsflag': 'object',\n",
    "            'Procedure code': 'object'\n",
    "        })\n",
    "\n",
    "        # Write the Dask DataFrame to Parquet file\n",
    "        df.to_parquet(output_file_path, engine='pyarrow', write_index=False)\n",
    "        \n",
    "    # Find directories that end with .parquet\n",
    "    parquet_directories = [directory for directory in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, directory)) and directory.endswith('.parquet')]\n",
    "    # Calculate total file sizes for each parquet directory\n",
    "    parquet_file_sizes = []\n",
    "    for directory in parquet_directories:\n",
    "        parquet_dir_path = os.path.join(current_directory, directory)\n",
    "        total_size = sum(os.path.getsize(os.path.join(parquet_dir_path, file)) for file in os.listdir(parquet_dir_path) if file.endswith('.parquet'))\n",
    "        parquet_file_sizes.append(total_size)\n",
    "    \n",
    "    # Update the 'Parquet Size' column in file_sizes DataFrame\n",
    "    file_sizes.loc[file_sizes['Filename'].isin([directory.replace('.parquet', '') for directory in parquet_directories]), 'Parquet Size'] = parquet_file_sizes\n",
    "    \n",
    "    print(\"CSV to Parquet conversion completed.\")\n",
    "\n",
    "    return file_sizes\n",
    "\n",
    "def calculate_reduction(file_sizes):\n",
    "    # Calculate % reduction\n",
    "    file_sizes['%Reduction'] = ((file_sizes['CSV Size'] - file_sizes['Parquet Size']) / file_sizes['CSV Size']) * 100\n",
    "\n",
    "    # Convert file sizes from bytes to megabytes (MB)\n",
    "    file_sizes['CSV Size [MB]'] = file_sizes['CSV Size'] / (1024 * 1024)  # Convert bytes to megabytes\n",
    "    file_sizes['Parquet Size [MB]'] = file_sizes['Parquet Size'] / (1024 * 1024)  # Convert bytes to megabytes\n",
    "\n",
    "    # Rearrange the columns\n",
    "    file_sizes = file_sizes[['Filename', 'CSV Size [MB]', 'Parquet Size [MB]', '%Reduction']]\n",
    "\n",
    "    return file_sizes\n",
    "\n",
    "# Get file sizes\n",
    "file_sizes = convert_csv_to_parquet(current_directory)\n",
    "file_sizes = calculate_reduction(file_sizes)\n",
    "file_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89610ad1",
   "metadata": {},
   "source": [
    "## Visualizing the Dataframes\n",
    "I made a widget earlier to rapidly see the pandas dataframes. I've adapted so that it can be used with dask dataframes and the new file structure as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "21ec2af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4217d30d4c914ecbafedc37ae59d947a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Select Value:', layout=Layout(width='400px'), max=14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa620f7089bb479a82a304dd60de167c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find directories that end with .parquet\n",
    "parquet_directories = [directory for directory in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, directory)) and directory.endswith('.parquet')]\n",
    "\n",
    "# Create a slider widget to control the value of val\n",
    "slider = widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=len(parquet_directories) - 1,\n",
    "    step=1,\n",
    "    description='Select Value:',\n",
    "    value=val,\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Create an output widget to display the DataFrame\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to update the Dask DataFrame based on the selected value\n",
    "def update_dataframe(value):\n",
    "    global val\n",
    "    val = value  # Update the value of val\n",
    "    parquet_directory = parquet_directories[val]\n",
    "    df = dd.read_parquet(f'{parquet_directory}/*.parquet')\n",
    "    file_name = parquet_directory.split('/')[-1] \n",
    "    \n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"File Name:\", file_name)\n",
    "        print(\"Number of rows:\", len(df))\n",
    "        print(\"Number of columns:\", len(df.columns))\n",
    "        display(df.head())\n",
    "\n",
    "# Call the update_dataframe function to initialize the Dask DataFrame\n",
    "update_dataframe(val)\n",
    "\n",
    "# Create an observer for the slider widget\n",
    "def slider_observer(change):\n",
    "    update_dataframe(change.new)\n",
    "\n",
    "# Add the observer to the slider widget\n",
    "slider.observe(slider_observer, names='value')\n",
    "\n",
    "# Display the slider widget and the output widget\n",
    "display(slider)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ef01f2",
   "metadata": {},
   "source": [
    "## Load the Training Data\n",
    "Now that I have these routines in place, I'm going to attempt to load the provided training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "85715eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: D:\\VCHAMPS Data\\Train Data\n",
      "['D:\\\\VCHAMPS Data\\\\Train Data\\\\conditions_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\death_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\demographics_event_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\demographics_static_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\ed_visits_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\immunization_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\inpatient_admissions_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\inpatient_location_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\inpatient_specialty_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\lab_results_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\measurements_blood_pressure_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\measurements_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\medications_administered_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\medications_ordered_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\outpatient_visits_train.csv', 'D:\\\\VCHAMPS Data\\\\Train Data\\\\procedures_train.csv']\n",
      "CSV to Parquet conversion completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>CSV Size [MB]</th>\n",
       "      <th>Parquet Size [MB]</th>\n",
       "      <th>%Reduction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>conditions_train</td>\n",
       "      <td>9830.91395</td>\n",
       "      <td>1789.354671</td>\n",
       "      <td>81.798695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>death_train</td>\n",
       "      <td>4.944784</td>\n",
       "      <td>3.228118</td>\n",
       "      <td>34.716707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>demographics_event_train</td>\n",
       "      <td>8.691981</td>\n",
       "      <td>4.468786</td>\n",
       "      <td>48.587254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>demographics_static_train</td>\n",
       "      <td>7.080438</td>\n",
       "      <td>1.652361</td>\n",
       "      <td>76.663012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ed_visits_train</td>\n",
       "      <td>69.029263</td>\n",
       "      <td>21.160005</td>\n",
       "      <td>69.346327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>immunization_train</td>\n",
       "      <td>232.495247</td>\n",
       "      <td>51.745118</td>\n",
       "      <td>77.74358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>inpatient_admissions_train</td>\n",
       "      <td>124.159206</td>\n",
       "      <td>24.052683</td>\n",
       "      <td>80.627548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>inpatient_location_train</td>\n",
       "      <td>78.85446</td>\n",
       "      <td>31.519588</td>\n",
       "      <td>60.028147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>inpatient_specialty_train</td>\n",
       "      <td>147.459064</td>\n",
       "      <td>54.221644</td>\n",
       "      <td>63.229358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lab_results_train</td>\n",
       "      <td>32755.922693</td>\n",
       "      <td>6416.455462</td>\n",
       "      <td>80.411312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>measurements_blood_pressure_train</td>\n",
       "      <td>1605.742848</td>\n",
       "      <td>592.246315</td>\n",
       "      <td>63.116989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>measurements_train</td>\n",
       "      <td>10050.849303</td>\n",
       "      <td>2221.664376</td>\n",
       "      <td>77.895755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>medications_administered_train</td>\n",
       "      <td>12653.11165</td>\n",
       "      <td>3227.639886</td>\n",
       "      <td>74.491335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>medications_ordered_train</td>\n",
       "      <td>9933.397246</td>\n",
       "      <td>2717.330762</td>\n",
       "      <td>72.644497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>outpatient_visits_train</td>\n",
       "      <td>9706.954212</td>\n",
       "      <td>2093.376971</td>\n",
       "      <td>78.434255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>procedures_train</td>\n",
       "      <td>22624.190149</td>\n",
       "      <td>2463.783955</td>\n",
       "      <td>89.109957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Filename CSV Size [MB] Parquet Size [MB]  \\\n",
       "0                    conditions_train    9830.91395       1789.354671   \n",
       "1                         death_train      4.944784          3.228118   \n",
       "2            demographics_event_train      8.691981          4.468786   \n",
       "3           demographics_static_train      7.080438          1.652361   \n",
       "4                     ed_visits_train     69.029263         21.160005   \n",
       "5                  immunization_train    232.495247         51.745118   \n",
       "6          inpatient_admissions_train    124.159206         24.052683   \n",
       "7            inpatient_location_train      78.85446         31.519588   \n",
       "8           inpatient_specialty_train    147.459064         54.221644   \n",
       "9                   lab_results_train  32755.922693       6416.455462   \n",
       "10  measurements_blood_pressure_train   1605.742848        592.246315   \n",
       "11                 measurements_train  10050.849303       2221.664376   \n",
       "12     medications_administered_train   12653.11165       3227.639886   \n",
       "13          medications_ordered_train   9933.397246       2717.330762   \n",
       "14            outpatient_visits_train   9706.954212       2093.376971   \n",
       "15                   procedures_train  22624.190149       2463.783955   \n",
       "\n",
       "   %Reduction  \n",
       "0   81.798695  \n",
       "1   34.716707  \n",
       "2   48.587254  \n",
       "3   76.663012  \n",
       "4   69.346327  \n",
       "5    77.74358  \n",
       "6   80.627548  \n",
       "7   60.028147  \n",
       "8   63.229358  \n",
       "9   80.411312  \n",
       "10  63.116989  \n",
       "11  77.895755  \n",
       "12  74.491335  \n",
       "13  72.644497  \n",
       "14  78.434255  \n",
       "15  89.109957  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to the desired directory\n",
    "directory_path = r'D:\\VCHAMPS Data\\Train Data'\n",
    "\n",
    "# Change the current working directory to the desired directory\n",
    "os.chdir(directory_path)\n",
    "\n",
    "# Verify the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current working directory: {current_directory}\")\n",
    "\n",
    "#Define data location\n",
    "path2data = r'D:\\VCHAMPS Data\\Quality Check'\n",
    "#Load the .csv files into memory\n",
    "csv_files  = load_csvs(directory_path)\n",
    "print(csv_files)\n",
    "\n",
    "# Get file sizes\n",
    "file_sizes = convert_csv_to_parquet(current_directory)\n",
    "file_sizes = calculate_reduction(file_sizes)\n",
    "file_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1f149b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of CSV Size [GB]: 107.25956689100713\n",
      "Sum of Parquet Size [GB]: 21.204981152899563\n",
      "Percent Reduction [GB]: 80.23%\n"
     ]
    }
   ],
   "source": [
    "# Convert the sum of sizes from MB to GB\n",
    "csv_size_sum_gb = csv_size_sum / 1024\n",
    "parquet_size_sum_gb = parquet_size_sum / 1024\n",
    "\n",
    "# Calculate the percent reduction in GB\n",
    "percent_reduction_gb = ((csv_size_sum_gb - parquet_size_sum_gb) / csv_size_sum_gb) * 100\n",
    "\n",
    "print(\"Sum of CSV Size [GB]:\", csv_size_sum_gb)\n",
    "print(\"Sum of Parquet Size [GB]:\", parquet_size_sum_gb)\n",
    "print(\"Percent Reduction [GB]: {:.2f}%\".format(percent_reduction_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7b247dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b306aef7ae4d63a93c4d8bf5b134e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=0, description='Select Value:', layout=Layout(width='400px'), max=15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dc80b7b771d4a64baa46430fabeff4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find directories that end with .parquet\n",
    "parquet_directories = [directory for directory in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, directory)) and directory.endswith('.parquet')]\n",
    "\n",
    "# Create a slider widget to control the value of val\n",
    "slider = widgets.IntSlider(\n",
    "    min=0,\n",
    "    max=len(parquet_directories) - 1,\n",
    "    step=1,\n",
    "    description='Select Value:',\n",
    "    value=val,\n",
    "    layout={'width': '400px'}\n",
    ")\n",
    "\n",
    "# Create an output widget to display the DataFrame\n",
    "output = widgets.Output()\n",
    "\n",
    "# Function to update the Dask DataFrame based on the selected value\n",
    "def update_dataframe(value):\n",
    "    global val\n",
    "    val = value  # Update the value of val\n",
    "    parquet_directory = parquet_directories[val]\n",
    "    df = dd.read_parquet(f'{parquet_directory}/*.parquet')\n",
    "    file_name = parquet_directory.split('/')[-1] \n",
    "    \n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        print(\"File Name:\", file_name)\n",
    "        print(\"Number of rows:\", len(df))\n",
    "        print(\"Number of columns:\", len(df.columns))\n",
    "        display(df.head())\n",
    "\n",
    "# Call the update_dataframe function to initialize the Dask DataFrame\n",
    "update_dataframe(val)\n",
    "\n",
    "# Create an observer for the slider widget\n",
    "def slider_observer(change):\n",
    "    update_dataframe(change.new)\n",
    "\n",
    "# Add the observer to the slider widget\n",
    "slider.observe(slider_observer, names='value')\n",
    "\n",
    "# Display the slider widget and the output widget\n",
    "display(slider)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893831ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:vchamps]",
   "language": "python",
   "name": "conda-env-vchamps-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

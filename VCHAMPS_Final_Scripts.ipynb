{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQK+aBFflVtFvkPtDCh7yN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victormurcia/VCHAMPS/blob/main/VCHAMPS_Final_Scripts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1. Libraries needed"
      ],
      "metadata": {
        "id": "CZDPJSXlYHAH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryMh65QKreM3"
      },
      "outputs": [],
      "source": [
        "#General utilities\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm  # Import tqdm for the progress bar\n",
        "import glob,shutil,os,warnings,math,time,sys,re,math\n",
        "import dask.dataframe as dd\n",
        "\n",
        "#For Slider viz\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output,HTML\n",
        "\n",
        "#DataPrep for Quick EDA\n",
        "#!pip install --q dataprep\n",
        "#from dataprep.eda import create_report\n",
        "\n",
        "#Enable data to be extracted and downloaded from my Google Drive\n",
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#For performing UTC normalization on datetime columns based on the STATE column\n",
        "import pytz\n",
        "\n",
        "#For UUID generation\n",
        "import uuid\n",
        "\n",
        "#For ML stuff\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix, accuracy_score, classification_report,auc,f1_score,recall_score,precision_score,make_scorer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV,train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "#!pip install eli5\n",
        "#import eli5\n",
        "#import shap"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Load CSVs"
      ],
      "metadata": {
        "id": "Wkf1oCbhZjbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csvs(path2data: str) -> List[str]:\n",
        "  \"\"\"\n",
        "  Load and return a list of CSV file paths from the specified directory.\n",
        "\n",
        "  Args:\n",
        "      path2data (str): The directory path containing the CSV files.\n",
        "\n",
        "  Returns:\n",
        "      List[str]: A list of CSV file paths.\n",
        "\n",
        "  \"\"\"\n",
        "  csv_files = glob.glob(path2data + '/*.csv')\n",
        "  return csv_files\n",
        "\n",
        "def make_df_list(csv_files: List[str]) -> List[pd.DataFrame]:\n",
        "  \"\"\"\n",
        "  Read CSV files from the provided list of file paths and return a list of DataFrames.\n",
        "\n",
        "  Args:\n",
        "      csv_files (List[str]): A list of CSV file paths.\n",
        "\n",
        "  Returns:\n",
        "      List[pd.DataFrame]: A list of DataFrames read from the CSV files.\n",
        "\n",
        "  \"\"\"\n",
        "  df_list = []\n",
        "  # Read the CSV file\n",
        "  for csv in csv_files:\n",
        "    df = pd.read_csv(csv)\n",
        "    df_list.append(df)\n",
        "\n",
        "  return df_list\n",
        "\n",
        "def clean_filenames(csv_files: List[str]) -> List[str]:\n",
        "  \"\"\"\n",
        "  Clean the file names by removing directory path and the .csv extension.\n",
        "\n",
        "  Args:\n",
        "      csv_files (List[str]): A list of CSV file paths.\n",
        "\n",
        "  Returns:\n",
        "      List[str]: A list of cleaned file names without directory path and file extension.\n",
        "\n",
        "  \"\"\"\n",
        "  #Get list of file names without directory junk and remove .csv extension from name\n",
        "  file_names = []\n",
        "\n",
        "  for file_path in csv_files:\n",
        "      file_name = os.path.basename(file_path)  # Get the file name with extension\n",
        "      file_name = os.path.splitext(file_name)[0]  # Remove the file extension\n",
        "      file_names.append(file_name)\n",
        "  return file_names\n",
        "\n",
        "# Specify the path to the desired directory\n",
        "directory_path = r'D:\\VCHAMPS Data\\Quality Check'\n",
        "\n",
        "# Change the current working directory to the desired directory\n",
        "os.chdir(directory_path)\n",
        "\n",
        "# Verify the current working directory\n",
        "current_directory = os.getcwd()\n",
        "print(f\"Current working directory: {current_directory}\")\n",
        "#Define data location\n",
        "path2data = r'D:\\VCHAMPS Data\\Quality Check'\n",
        "#Load the .csv files into memory\n",
        "csv_files  = load_csvs(path2data)\n",
        "print(csv_files)\n",
        "#Create list of dataframes from csvs\n",
        "df_list_csv    = make_df_list(csv_files)\n",
        "#Clean the names of .csv files\n",
        "csv_file_names = clean_filenames(csv_files)"
      ],
      "metadata": {
        "id": "Eg6lCtW5YF_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3. Conversion of CSVs to Parquet"
      ],
      "metadata": {
        "id": "cKTR7FwLYdV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_csv_to_parquet(current_directory):\n",
        "    # Create an empty DataFrame to store the file sizes\n",
        "    file_sizes = pd.DataFrame(columns=['Filename', 'CSV Size', 'Parquet Size'])\n",
        "\n",
        "    # Find all CSV files in the input directory\n",
        "    csv_files = [file for file in os.listdir(current_directory) if file.endswith('.csv')]\n",
        "\n",
        "    # Process each CSV file\n",
        "    for i, csv_file in enumerate(csv_files):\n",
        "        # Create the file paths\n",
        "        input_file_path = os.path.join(current_directory, csv_file)\n",
        "        output_file_name = f\"{csv_file.replace('.csv', '')}.parquet\"\n",
        "        output_file_path = os.path.join(current_directory, output_file_name)\n",
        "\n",
        "        # Get the size of the CSV file\n",
        "        csv_file_size = os.path.getsize(input_file_path)\n",
        "\n",
        "        # Append the file size to the DataFrame\n",
        "        file_sizes = pd.concat([file_sizes, pd.DataFrame({\n",
        "            'Filename': [csv_file.replace('.csv', '')],\n",
        "            'CSV Size': [csv_file_size],\n",
        "            'Parquet Size': [None]  # Initialize with None\n",
        "        })], ignore_index=True)\n",
        "\n",
        "        # Read the CSV file as a Dask DataFrame\n",
        "        df = dd.read_csv(input_file_path, dtype={\n",
        "            'Administered elsewhere': 'object',\n",
        "            'Dose unit': 'object',\n",
        "            'Result textual': 'object',\n",
        "            'Administration end date': 'object',\n",
        "            'Agentorangeflag': 'object',\n",
        "            'Combatflag': 'object',\n",
        "            'Ionizingradiationflag': 'object',\n",
        "            'Swasiaconditionsflag': 'object',\n",
        "            'Procedure code': 'object'\n",
        "        })\n",
        "\n",
        "        # Write the Dask DataFrame to Parquet file\n",
        "        df.to_parquet(output_file_path, engine='pyarrow', write_index=False)\n",
        "\n",
        "    # Find directories that end with .parquet\n",
        "    parquet_directories = [directory for directory in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, directory)) and directory.endswith('.parquet')]\n",
        "    # Calculate total file sizes for each parquet directory\n",
        "    parquet_file_sizes = []\n",
        "    for directory in parquet_directories:\n",
        "        parquet_dir_path = os.path.join(current_directory, directory)\n",
        "        total_size = sum(os.path.getsize(os.path.join(parquet_dir_path, file)) for file in os.listdir(parquet_dir_path) if file.endswith('.parquet'))\n",
        "        parquet_file_sizes.append(total_size)\n",
        "\n",
        "    # Update the 'Parquet Size' column in file_sizes DataFrame\n",
        "    file_sizes.loc[file_sizes['Filename'].isin([directory.replace('.parquet', '') for directory in parquet_directories]), 'Parquet Size'] = parquet_file_sizes\n",
        "\n",
        "    print(\"CSV to Parquet conversion completed.\")\n",
        "\n",
        "    return file_sizes\n",
        "\n",
        "def calculate_reduction(file_sizes):\n",
        "    # Calculate % reduction\n",
        "    file_sizes['%Reduction'] = ((file_sizes['CSV Size'] - file_sizes['Parquet Size']) / file_sizes['CSV Size']) * 100\n",
        "\n",
        "    # Convert file sizes from bytes to megabytes (MB)\n",
        "    file_sizes['CSV Size [MB]'] = file_sizes['CSV Size'] / (1024 * 1024)  # Convert bytes to megabytes\n",
        "    file_sizes['Parquet Size [MB]'] = file_sizes['Parquet Size'] / (1024 * 1024)  # Convert bytes to megabytes\n",
        "\n",
        "    # Rearrange the columns\n",
        "    file_sizes = file_sizes[['Filename', 'CSV Size [MB]', 'Parquet Size [MB]', '%Reduction']]\n",
        "\n",
        "    return file_sizes\n",
        "\n",
        "# Get file sizes\n",
        "file_sizes = convert_csv_to_parquet(current_directory)\n",
        "file_sizes = calculate_reduction(file_sizes)"
      ],
      "metadata": {
        "id": "Y-bydlDAYq3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4. ICD Mapping Functions and Basic Data Clean Up"
      ],
      "metadata": {
        "id": "wkQYrhhdipxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_icd_codes(path):\n",
        "    icd_codes = pd.read_csv(path).drop('Unnamed: 0', axis = 1)\n",
        "    return icd_codes\n",
        "\n",
        "# Function to split semicolon separated diagnoses into a list of diagnoses\n",
        "def split_diagnoses(diagnoses):\n",
        "    return [dx.lower().replace(\",\", \"\").strip() for dx in diagnoses.split(';')]\n",
        "\n",
        "def process_diagnoses_dataframes(dataframes,icd_codes):\n",
        "    final_df_list = []\n",
        "    for idx, df in enumerate(dataframes):\n",
        "        # Get the columns containing diagnoses (columns with 'icd10' in their name)\n",
        "        diagnosis_columns = [col for col in df.columns if 'icd10' in col.lower()]\n",
        "        print(idx, diagnosis_columns)\n",
        "        df.compute()\n",
        "\n",
        "        # Create a new column with a single list of diagnoses\n",
        "        for i in range(len(diagnosis_columns)):\n",
        "            if i == 0:\n",
        "              df['diagnosis'] = df[diagnosis_columns[i]].apply(split_diagnoses)\n",
        "            else:\n",
        "              df['diagnosis'] = df['diagnosis'] + df[diagnosis_columns[i]].apply(split_diagnoses)\n",
        "\n",
        "        # Explode the columns of lists so each row has a single diagnosis\n",
        "        df = df.explode('diagnosis')\n",
        "        df.reset_index(drop = True)\n",
        "        # Merge to icd_codes csv\n",
        "        df = df.merge(icd_codes, left_on='diagnosis', right_on='description', how='left')\n",
        "        df = df.drop(columns = diagnosis_columns + ['description'])\n",
        "        final_df_list.append(df)\n",
        "    return final_df_list\n",
        "\n",
        "def process_diagnoses_dataframe(df, icd_codes):\n",
        "    # Get the columns containing diagnoses (columns with 'icd10' in their name)\n",
        "    diagnosis_columns = [col for col in df.columns if 'icd10' in col.lower()]\n",
        "    print(diagnosis_columns)\n",
        "    df.compute()\n",
        "\n",
        "    # Create a new column with a single list of diagnoses\n",
        "    for i in range(len(diagnosis_columns)):\n",
        "        if i == 0:\n",
        "            df['diagnosis'] = df[diagnosis_columns[i]].apply(split_diagnoses)\n",
        "        else:\n",
        "            df['diagnosis'] = df['diagnosis'] + df[diagnosis_columns[i]].apply(split_diagnoses)\n",
        "\n",
        "    # Explode the columns of lists so each row has a single diagnosis\n",
        "    df = df.explode('diagnosis')\n",
        "    df.reset_index(drop=True)\n",
        "\n",
        "    # Merge with icd_codes dataframe\n",
        "    df = df.merge(icd_codes, left_on='diagnosis', right_on='description', how='left')\n",
        "    df = df.drop(columns=diagnosis_columns + ['description'])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def load_parquet_files(folder_path):\n",
        "    # Get a list of all files in the current directory\n",
        "    all_files = [f.path for f in os.scandir(folder_path) if f.is_file()]\n",
        "\n",
        "    # Get a list of all subdirectories (dataset folders) in the current directory\n",
        "    dataset_folders = [f.path for f in os.scandir(folder_path) if f.is_dir()]\n",
        "\n",
        "    # Initialize an empty list to store the DataFrames\n",
        "    dataframes = []\n",
        "\n",
        "    # Load Parquet files in the current directory\n",
        "    for file in all_files:\n",
        "        if file.endswith('.parquet'):\n",
        "            df = dd.read_parquet(file, engine='pyarrow')\n",
        "            dataframes.append(df)\n",
        "\n",
        "    # Load Parquet files in the subfolders\n",
        "    for folder in dataset_folders:\n",
        "        df = dd.read_parquet(folder + '/**/*.parquet', engine='pyarrow')\n",
        "        dataframes.append(df)\n",
        "\n",
        "    return dataframes\n",
        "\n",
        "\n",
        "# Specify the path to the desired directory\n",
        "directory_path = r'/content/drive/MyDrive/VCHAMPS - Train Data -  UTC'\n",
        "\n",
        "# Change the current working directory to the desired directory\n",
        "os.chdir(directory_path)\n",
        "\n",
        "# Verify the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "print(f\"Current working directory: {cwd}\")\n",
        "dataframes = load_parquet_files(cwd)\n",
        "\n",
        "icd_codes = read_icd_codes('/content/drive/MyDrive/icd_codes_cc.csv')\n",
        "\n",
        "def preprocess_dataframe(df, col_name):\n",
        "    # Categorize a column\n",
        "    df[col_name] = df[col_name].astype('category')\n",
        "    df[col_name] = df[col_name].cat.as_known()  # Ensure known categories\n",
        "\n",
        "    # Get dummies for a specific column\n",
        "    dummies = dd.get_dummies(df[col_name])\n",
        "\n",
        "    # Convert dummy columns to bool data type\n",
        "    for col in dummies.columns:\n",
        "        dummies[col] = dummies[col].astype(bool)\n",
        "\n",
        "    # Concatenate the dummies with the original DataFrame\n",
        "    df_with_dummies = dd.concat([df, dummies], axis=1)\n",
        "\n",
        "    # Drop a column\n",
        "    new_df = df_with_dummies.drop(col_name, axis=1)\n",
        "\n",
        "    # Clear df and dummies from memory\n",
        "    del df, dummies\n",
        "\n",
        "    return new_df\n",
        "\n",
        "#Clean Up and Preprocess Conditions\n",
        "col_name = 'Condition type'\n",
        "df = dataframes[0] #This needs to be the condition dataframe!!\n",
        "conditions_df = preprocess_dataframe(df,col_name)\n",
        "conditions_df = conditions_df.dropna(subset=['Diagnosis sequence number or rank'])\n",
        "conditions_df = process_diagnoses_dataframe(conditions_df, icd_codes)\n",
        "conditions_df = conditions_df.drop(columns=['diagnosis','billable'])\n",
        "conditions_df = conditions_df.dropna(subset=['code','cc Status'])\n",
        "# Save the Dask DataFrame as Parquet\n",
        "conditions_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/conditions_train.parquet', engine='pyarrow')\n",
        "\n",
        "\n",
        "#Clean Up and Preprocess Demographics Event\n",
        "df = dataframes[1]\n",
        "dem_event_df = preprocess_dataframe(df,'Marital status')\n",
        "dem_event_df = preprocess_dataframe(dem_event_df,'Ruca category')\n",
        "dem_event_df = dem_event_df.drop(columns=['Not specified (no value)','Not specified'])\n",
        "# Save the Dask DataFrame as Parquet\n",
        "dem_event_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/demographics_event_df.parquet', engine='pyarrow')\n",
        "\n",
        "#Clean Up and Preprocess Demographics Static\n",
        "dem_static_df = dataframes[2]\n",
        "# Drop rows where 'Ethnicity' column contains 'Not specified'\n",
        "dem_static_df = dem_static_df[dem_static_df['Ethnicity'] != 'Not specified']\n",
        "dem_static_df = preprocess_dataframe(dem_static_df,'Gender')\n",
        "# Drop rows where 'Races' column contains '(Censored)'\n",
        "dem_static_df = dem_static_df[dem_static_df['Races'] != '(Censored)']\n",
        "dem_static_df = preprocess_dataframe(dem_static_df,'Races')\n",
        "# Define the value conversions\n",
        "value_conversions = {'Yes': 1, 'No': 0}\n",
        "# Convert specific values in the column\n",
        "dem_static_df['Veteran flag'] = dem_static_df['Veteran flag'].map(value_conversions, na_action='ignore')\n",
        "dem_static_df = dem_static_df.drop(columns=['Not specified (no value)'])\n",
        "# Save the Dask DataFrame as Parquet\n",
        "dem_static_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/demographics_static.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Death\n",
        "df = dataframes[3] #Death had no NaN values so I'll just keep it as is for now\n",
        "# Save the Dask DataFrame as Parquet\n",
        "df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/death.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning ED Visits\n",
        "ed_visits_df = dataframes[4]\n",
        "# Drop rows with NaN or NaT values from a specific column\n",
        "ed_visits_df = ed_visits_df.dropna(subset=['Discharge date ed'])\n",
        "ed_visits_df = process_diagnoses_dataframe(ed_visits_df, icd_codes)\n",
        "ed_visits_df = ed_visits_df.drop(columns=['diagnosis','billable'])\n",
        "ed_visits_df = ed_visits_df.dropna(subset=['code','cc Status'])\n",
        "# Assuming ed_visits_df is your Dask DataFrame\n",
        "ed_visits_df['Died during ed visit'] = ed_visits_df['Died during ed visit'].map({'Yes': 1, 'No': 0}, meta=('Died during ed visit'))\n",
        "# Save the Dask DataFrame as Parquet\n",
        "ed_visits_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/ed_visits.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning immunizations\n",
        "df = dataframes[5]\n",
        "df = df.drop(columns=['Dose quantity','Dose unit'])\n",
        "df = df.dropna(subset=['Cvx code'])\n",
        "df['Cvx code'] = df['Cvx code'].astype('int16')\n",
        "df['Series doses'] = df['Series doses'].replace('Not specified (no value)', 'NS')\n",
        "dummy_df  = pd.get_dummies(df['Series doses'], prefix='Series doses')\n",
        "df = df.merge(dummy_df, left_index=True, right_index=True)\n",
        "df = df.drop(columns=['Series doses'])\n",
        "# Save the Dask DataFrame as Parquet\n",
        "df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/immunizations.parquet', engine='pyarrow')\n",
        "\n",
        "#Cleaning inpatient admissions DF\n",
        "inpatient_df = dataframes[6]\n",
        "inpatient_df = process_diagnoses_dataframe(inpatient_df, icd_codes)\n",
        "inpatient_df['CV diagnosis'] = inpatient_df['CV diagnosis'].astype('int8')\n",
        "inpatient_df = inpatient_df.drop(columns=['Serviceconnectedflag','billable'])\n",
        "inpatient_df = inpatient_df.dropna(subset=['code','Outpatientreferralflag'])\n",
        "mode = inpatient_df['Agentorangeflag'].mode().iloc[0]\n",
        "inpatient_df['Agentorangeflag'] = inpatient_df['Agentorangeflag'].fillna(mode)\n",
        "inpatient_df = inpatient_df.dropna(subset=['Discharge date'])\n",
        "\n",
        "for col in ['Died during admission', 'Outpatientreferralflag', 'Agentorangeflag']:\n",
        "  inpatient_df = inpatient_df[~((inpatient_df[col].isna()) | (inpatient_df[col] == np.inf) | (inpatient_df[col] == -np.inf))]\n",
        "\n",
        "mapping = {'No': 0, 'Yes': 1}\n",
        "for col in ['Outpatientreferralflag', 'Agentorangeflag']:\n",
        "    print(col)\n",
        "    inpatient_df[col] = inpatient_df[col].map(mapping).astype('int8')\n",
        "\n",
        "dummy_df  = pd.get_dummies(inpatient_df['cc Status'], prefix='cc Status')\n",
        "inpatient_df = inpatient_df.merge(dummy_df, left_index=True, right_index=True)\n",
        "inpatient_df = inpatient_df.drop(columns=['cc Status'])\n",
        "inpatient_df = inpatient_df[~((inpatient_df['Admitting unit service'] == '(Censored)') | (inpatient_df['Admitting unit service'] == 'Not specified'))]\n",
        "inpatient_df = inpatient_df[~((inpatient_df['Discharging unit service'] == '(Censored)') | (inpatient_df['Discharging unit service'] == 'Not specified') | (inpatient_df['Discharging unit service'] == 'Not specified (no value)'))]\n",
        "inpatient_df = inpatient_df[~(inpatient_df['Discharge disposition'] == 'Not specified (no value)')]\n",
        "dummy_df  = pd.get_dummies(inpatient_df['Discharge disposition'], prefix='Discharge disposition')\n",
        "inpatient_df = inpatient_df.merge(dummy_df, left_index=True, right_index=True)\n",
        "inpatient_df = inpatient_df.drop(columns=['Discharge disposition'])\n",
        "# Save the Dask DataFrame as Parquet\n",
        "inpatient_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/inpatient_admissions.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Inpatient Location\n",
        "inpatient_location_df = dataframes[7]\n",
        "inpatient_location_df = inpatient_location_df.dropna(subset=['Location end date'])\n",
        "inpatient_location_df = inpatient_location_df[~((inpatient_location_df['Service'] == '(Censored)') | (inpatient_location_df['Service'] == 'Not specified') | (inpatient_location_df['Service'] == 'Not specified (no value)'))]\n",
        "\n",
        "mapping = {'No': 0, 'Yes': 1}\n",
        "inpatient_location_df['Died at location'] = inpatient_location_df['Died at location'].map(mapping).astype('int8')\n",
        "# Save the Dask DataFrame as Parquet\n",
        "inpatient_location_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/inpatient_location.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Inpatient Specialty\n",
        "inpatient_specialty_df = dataframes[8]\n",
        "inpatient_specialty_df = inpatient_specialty_df.dropna(subset=['Specialty end date'])\n",
        "inpatient_specialty_df = inpatient_specialty_df[~((inpatient_specialty_df['Specialty'] == '(Censored)') | (inpatient_specialty_df['Specialty'] == 'Not specified') | (inpatient_specialty_df['Specialty'] == 'Not specified (no value)'))]\n",
        "# Save the Dask DataFrame as Parquet\n",
        "inpatient_specialty_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/inpatient_specialty.parquet', engine='\n",
        "\n",
        "# Cleaning for Measurements DF\n",
        "measurements_df = dataframes[9]\n",
        "columns_list = [\"Age at measurement\", \"Measurement date\", \"Measurement\", \"Result numeric\"]\n",
        "measurements_df = measurements_df.dropna(subset = columns_list)\n",
        "measurements_df = measurements_df.drop(columns=['Result textual'])\n",
        "measurements_df = dd.from_pandas(measurements_df, npartitions=100)\n",
        "measurements_df['Measurement'] = measurements_df['Measurement'].astype('category')\n",
        "measurements_df['Measurement'] = measurements_df['Measurement'].cat.as_known()\n",
        "dummy_df = dd.get_dummies(measurements_df['Measurement'], prefix='Measurement')\n",
        "measurements_df = measurements_df.merge(dummy_df, left_index=True, right_index=True)\n",
        "measurements_df = measurements_df.drop(columns=['Measurement'])\n",
        "# Save the Dask DataFrame as Parquet\n",
        "measurements_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/measurements.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Lab Results DF\n",
        "lab_harm_df = pd.read_csv('/content/drive/MyDrive/labHarmonization.csv')\n",
        "filtered_lab_harm_df = lab_harm_df[lab_harm_df['concept'].notna()].sort_values('sd', ascending=False).drop(columns=['Unnamed: 0']).reset_index(drop=True)\n",
        "labs_to_keep = filtered_lab_harm_df['desc'].tolist()\n",
        "value_counts = filtered_lab_harm_df['unit'].value_counts()\n",
        "values_with_count_1 = value_counts[value_counts == 1]\n",
        "units_to_remove_list = values_with_count_1.index.tolist()\n",
        "\n",
        "lab_results_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Data -  UTC/lab_results_train.parquet/*.parquet')\n",
        "columns_list = [\"Age at lab test\", \"Lab test date\", \"Lab test\", \"Result numeric\",'Result units']\n",
        "lab_results_df = lab_results_df.dropna(subset = columns_list)\n",
        "filtered_lab_results_df = lab_results_df[lab_results_df['Lab test description'].isin(labs_to_keep)]\n",
        "filtered_lab_results_df = filtered_lab_results_df.reset_index(drop=True)\n",
        "dask_df = dd.from_pandas(filtered_lab_harm_df, npartitions=2)\n",
        "merged_df = dd.merge(filtered_lab_results_df, dask_df, left_on='Lab test description', right_on='desc')\n",
        "columns_to_convert = ['Count', 'Mean', 'sd', 'check', 'k', 'na', 'wbc', 'bicarb', 'pco2', 'hct', 'cr', 'hgb', 'lactate', 'ldh', 'ph', 'gfr', 'ast', 'alt', 'inr', 'a1c', 'ferritin', 'tropi', 'tropt', 'trophs', 'bnp', 'ntprobnp', 'tbili', 'ddimer', 'esr', 'crp', 'hscrp', 'meth_sc', 'meth_lvl', 'methadone_sc', 'methadone_lvl', 'cocaine_sc', 'cocaine_lvl']\n",
        "for column in columns_to_convert:\n",
        "    dask_df[column] = dask_df[column].astype('int8')\n",
        "merged_df['Mean'] = merged_df['Mean'].round(3)\n",
        "merged_df['sd'] = merged_df['sd'].round(3)\n",
        "merged_df = merged_df.dropna(subset = ['Result units','Result range'])\n",
        "split_columns = merged_df['Result range'].str.split(' - ')\n",
        "merged_df['range_min'] = split_columns.str[0]\n",
        "merged_df['range_max'] = split_columns.str[1]\n",
        "merged_df['range_min'] = merged_df['range_min'].astype(float)\n",
        "merged_df['range_max'] = merged_df['range_max'].astype(float)\n",
        "merged_df = merged_df.drop(columns = ['Result range'])\n",
        "merged_df = merged_df.drop(columns = ['Lab test description','Result units','check','Count'])\n",
        "columns_to_drop = ['k', 'na', 'wbc', 'bicarb', 'pco2', 'hct', 'cr', 'hgb', 'lactate', 'ldh', 'ph', 'gfr', 'ast', 'alt', 'inr', 'a1c', 'ferritin', 'tropi', 'tropt', 'trophs', 'bnp', 'ntprobnp', 'tbili', 'ddimer', 'esr', 'crp', 'hscrp', 'meth_sc', 'meth_lvl', 'methadone_sc', 'methadone_lvl', 'cocaine_sc', 'cocaine_lvl']\n",
        "merged_df = merged_df.drop(columns = columns_to_drop)\n",
        "merged_df = merged_df.dropna()\n",
        "merged_df = merged_df[(merged_df['Result numeric'] >= merged_df['Mean'] - 2 * merged_df['sd']) | (merged_df['Result numeric'] <= merged_df['Mean'])\n",
        "merged_df = merged_df.drop(columns = ['Mean','sd'])\n",
        "def remove_rows_with_values(df, column_name, values_list):\n",
        "    filtered_df = df[~df[column_name].isin(values_list)]\n",
        "    return filtered_df\n",
        "cleaned_df = remove_rows_with_values(merged_df, 'unit', units_to_remove_list)\n",
        "cleaned_df = cleaned_df.repartition(npartitions=100)\n",
        "cleaned_df['concept'] = cleaned_df['concept'].astype('category')\n",
        "cleaned_df['concept'] = cleaned_df['concept'].cat.as_known()\n",
        "# Save the Dask DataFrame as Parquet\n",
        "cleaned_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/lab_results.parquet', engine='pyarrow', row_group_size=100000, write_index=False)\n",
        "\n",
        "#Cleaning Measurements BP DF\n",
        "measurements_bp_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Data -  UTC/measurements_blood_pressure_train.parquet/*.parquet')\n",
        "measurements_bp_df = measurements_bp_df.dropna()\n",
        "measurements_bp_df = measurements_bp_df.compute()\n",
        "measurements_bp_df['Diastolic bp'] = measurements_bp_df['Diastolic bp'].astype('int16')\n",
        "measurements_bp_df['Systolic bp'] = measurements_bp_df['Systolic bp'].astype('int16')\n",
        "# Save the Dask DataFrame as Parquet\n",
        "measurements_bp_df.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/measurements_bp.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Medication Administered DF\n",
        "medications_administered_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Data -  UTC/medications_administered_train.parquet/*.parquet')\n",
        "medications_administered_df = medications_administered_df.compute()\n",
        "medications_administered_df = medications_administered_df[~(medications_administered_df['Administration status'] == 'Not specified (no value)')]\n",
        "dose_form_list = medications_administered_df['Dose form'].value_counts()\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose form'] == 'miscellaneous') | (medications_administered_df['Dose form'] == '*unknown at this time*'))]\n",
        "medications_administered_df = medications_administered_df.dropna(subset = ['Dose administered','Dose unit administered'])\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Administered medication atc 5'] == '(Censored)'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Administered medication atc 5'] == 'Not specified'))]\n",
        "\n",
        "magd = pd.read_csv('/content/drive/MyDrive/medications_administered_grouped_data.csv')\n",
        "dose_unit_admin_list = medications_administered_df['Dose unit administered'].value_counts()\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == '.'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'as ordered'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'moderate amount'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'small amt'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'mod amt'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'as directed'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'ad'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'liberal amount'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'moderate'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'per order'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'liberally'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'ok'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'ao'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'lib amt'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'liberal'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'sm amount'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'a small amount'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'suff'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'some'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'as order'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'as ord'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'a'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'ade'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'dose'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'as rx'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'mod. amt.'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'suff amount'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'smamt'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'as'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'p'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'small amnt'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'adequate amount'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'moderate amt.'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'sm amnt'))]\n",
        "medications_administered_df = medications_administered_df[~((medications_administered_df['Dose unit administered'] == 'aso'))]\n",
        "medications_administered_df['Administration status'] = medications_administered_df['Administration status'].astype('category')\n",
        "medications_administered_ddf  = dd.from_pandas(medications_administered_df, npartitions=100)\n",
        "# Save the Dask DataFrame as Parquet\n",
        "medications_administered_ddf.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/medications_administered.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Medication Ordered DF\n",
        "medications_ordered_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Data -  UTC/medications_ordered_train.parquet/*.parquet')\n",
        "medications_ordered_df = medications_ordered_df.compute()\n",
        "medications_ordered_df['Order status'] = medications_ordered_df['Order status'].astype('category')\n",
        "medications_ordered_df = medications_ordered_df[~((medications_ordered_df['Ordered medication atc 5'] == '(Censored)'))]\n",
        "medications_ordered_df = medications_ordered_df[~((medications_ordered_df['Ordered medication atc 5'] == 'Not specified'))]\n",
        "medications_ordered_ddf = dd.from_pandas(medications_ordered_df, npartitions=100)\n",
        "# Save the Dask DataFrame as Parquet\n",
        "medications_ordered_ddf.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/medications_ordered.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning outpatient visits\n",
        "outpatient_visits_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Data -  UTC/outpatient_visits_train.parquet/*.parquet')\n",
        "cols_to_convert = ['Agentorangeflag','Combatflag','Ionizingradiationflag','Serviceconnectedflag','Swasiaconditionsflag']\n",
        "for col in cols_to_convert:\n",
        "  outpatient_visits_df[col] = outpatient_visits_df[col].astype('category')\n",
        "  outpatient_visits_df[col] = outpatient_visits_df[col].cat.as_known()\n",
        "outpatient_visits_df = process_diagnoses_dataframe(outpatient_visits_df, icd_codes)\n",
        "outpatient_visits_df = outpatient_visits_df.drop(columns = ['billable'])\n",
        "outpatient_visits_df = outpatient_visits_df.dropna(subset = ['code','diagnosis'])\n",
        "outpatient_visits_df = outpatient_visits_df.compute()\n",
        "outpatient_visits_df = outpatient_visits_df[~((outpatient_visits_df['Stop code'] == '(Censored)'))]\n",
        "outpatient_visits_df = outpatient_visits_df[~((outpatient_visits_df['Stop code'] == '100 NOT USED'))]\n",
        "outpatient_visits_df = outpatient_visits_df[~((outpatient_visits_df['Stop code'] == 'Not specified (no value)'))]\n",
        "outpatient_visits_df['cc Status'] = outpatient_visits_df['cc Status'].astype('category')\n",
        "outpatient_visits_ddf = dd.from_pandas(outpatient_visits_df, npartitions=80)\n",
        "# Save the Dask DataFrame as Parquet\n",
        "outpatient_visits_ddf.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/outpatient_visits.parquet', engine='pyarrow')\n",
        "\n",
        "# Cleaning Up Procedures DF\n",
        "procedures_df = dd.read_parquet('/content/drive/MyDrive/VCHAMPS - Train Data -  UTC/procedures_train.parquet/*.parquet')\n",
        "procedures_df = procedures_df.compute()\n",
        "procedures_df = procedures_df.dropna()\n",
        "procedures_ddf = dd.from_pandas(procedures_df, npartitions=80)\n",
        "# Save the Dask DataFrame as Parquet\n",
        "procedures_ddf.to_parquet('/content/drive/MyDrive/VCHAMPS - Train Cleaned/procedures.parquet', engine='pyarrow')"
      ],
      "metadata": {
        "id": "3Slv-b_xiv-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 5. Encounter Mapping"
      ],
      "metadata": {
        "id": "-kQN1_oljppf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_encounter_id_vectorized(row, age_col, date_col):\n",
        "    \"\"\"\n",
        "    Maps the encounter ID for a given row based on matching criteria in different dataframes.\n",
        "\n",
        "    Args:\n",
        "        row (pandas.Series): The row containing the data to be matched.\n",
        "        age_col (str): The column name for the patient's age in the row.\n",
        "        date_col (str): The column name for the date to match in the row.\n",
        "\n",
        "    Returns:\n",
        "        str: The matched encounter ID if found, or a newly generated UUID if no match is found.\n",
        "\n",
        "    \"\"\"\n",
        "    patient_id = row['Internalpatientid']\n",
        "    patient_age = row[age_col]\n",
        "    date_to_match = row[date_col]\n",
        "\n",
        "    filtered_ed_visits = ed_visits_df[ed_visits_df['Internalpatientid'] == patient_id]\n",
        "    ed_visit_match = (filtered_ed_visits['Ed visit start date'] <= date_to_match) & (filtered_ed_visits['Discharge date ed'] >= date_to_match) & (filtered_ed_visits['Age at ed visit'] <= patient_age)\n",
        "    if ed_visit_match.any():\n",
        "        return filtered_ed_visits.loc[ed_visit_match, 'Encounter ID'].iloc[0]\n",
        "\n",
        "    filtered_inpatient_admissions = inpatient_admissions_df[inpatient_admissions_df['Internalpatientid'] == patient_id]\n",
        "    inpatient_match = (filtered_inpatient_admissions['Admission date'] <= date_to_match) & (filtered_inpatient_admissions['Discharge date'] >= date_to_match) & (filtered_inpatient_admissions['Age at admission'] <= patient_age)\n",
        "    if inpatient_match.any():\n",
        "        return filtered_inpatient_admissions.loc[inpatient_match, 'Encounter ID'].iloc[0]\n",
        "\n",
        "    filtered_outpatient_visits = outpatient_visits_df[outpatient_visits_df['Internalpatientid'] == patient_id]\n",
        "    outpatient_match = (filtered_outpatient_visits['Visit start date'] <= date_to_match) & (filtered_outpatient_visits['Visit End Date'] >= date_to_match) & (filtered_outpatient_visits['Age at visit'] <= patient_age)\n",
        "    if outpatient_match.any():\n",
        "        return filtered_outpatient_visits.loc[outpatient_match, 'Encounter ID'].iloc[0]\n",
        "\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "def process_and_save_chunks(df, save_path,df_name,age_col,date_col):\n",
        "    \"\"\"\n",
        "    Process the input DataFrame (df) in chunks, calculate encounter IDs,\n",
        "    and save the results to Parquet files.\n",
        "\n",
        "    Parameters:\n",
        "        df (pandas.DataFrame): The DataFrame containing the data to process.\n",
        "        save_path (str): The path to save the Parquet files.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Define the chunk size\n",
        "    chunk_size = 100000\n",
        "\n",
        "    # Calculate the number of chunks\n",
        "    num_chunks = math.ceil(len(df) / chunk_size)\n",
        "\n",
        "    # Create an empty list to store the encounter IDs\n",
        "    encounter_ids = []\n",
        "\n",
        "    # Iterate over chunks\n",
        "    for i in range(num_chunks):\n",
        "        start_idx = i * chunk_size\n",
        "        end_idx = (i + 1) * chunk_size\n",
        "\n",
        "        # Get the chunk of dataframe\n",
        "        chunk_df = df[start_idx:end_idx]\n",
        "\n",
        "        # Process the chunk and track progress using tqdm\n",
        "        for _, row in tqdm(chunk_df.iterrows(), total=chunk_df.shape[0], desc=f\"Processing Chunk {i+1}/{num_chunks}\"):\n",
        "            encounter_id = map_encounter_id_vectorized(row, age_col, date_col)\n",
        "            encounter_ids.append(encounter_id)\n",
        "\n",
        "        # Create a new DataFrame with the chunk results\n",
        "        chunk_results_df = chunk_df.copy()\n",
        "        chunk_results_df['Encounter ID'] = encounter_ids[start_idx:end_idx]\n",
        "\n",
        "        # Save the results of the chunk to Parquet file\n",
        "        chunk_results_df.to_parquet(f'{save_path}/{df_name}{i+1}.parquet', index=False)\n",
        "\n",
        "# Specify the path to the desired directory\n",
        "directory_path = r'/content/drive/MyDrive/VCHAMPS - Train Cleaned'\n",
        "\n",
        "# Change the current working directory to the desired directory\n",
        "os.chdir(directory_path)\n",
        "\n",
        "# Verify the current working directory\n",
        "cwd = os.getcwd()\n",
        "\n",
        "print(f\"Current working directory: {cwd}\")\n",
        "\n",
        "mapped_dir = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped'\n",
        "#Load the Dataframes\n",
        "ed_visits_df            = dd.read_parquet(mapped_dir + '/ed_visits.parquet')\n",
        "inpatient_admissions_df = dd.read_parquet(mapped_dir + '/inpatient_admissions.parquet')\n",
        "outpatient_visits_df    = dd.read_parquet(mapped_dir + '/outpatient_visits.parquet')\n",
        "\n",
        "ed_visits_df = ed_visits_df.compute()\n",
        "inpatient_admissions_df = inpatient_admissions_df.compute()\n",
        "outpatient_visits_df = outpatient_visits_df.compute()\n",
        "\n",
        "#Map Lab results\n",
        "lab_results_df = dd.read_parquet(directory_path + '/lab_results.parquet/*.parquet')\n",
        "lab_results_df = lab_results_df.compute()\n",
        "lab_results_df = lab_results_df.reset_index(drop=True)\n",
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/lab_results'\n",
        "process_and_save_chunks(lab_results_df, save_path,df_name,'Age at lab test', 'Lab test date')\n",
        "\n",
        "#Map conditions\n",
        "conditions_df = dd.read_parquet(directory_path + '/conditions.parquet/*.parquet')\n",
        "conditions_df = conditions_df.compute()\n",
        "conditions_df = conditions_df.reset_index(drop=True)\n",
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/conditions'\n",
        "process_and_save_chunks(conditions_df, save_path,df_name,'Age at condition', 'Condition documented date')\n",
        "\n",
        "#Map conditions\n",
        "demographics_event_df = dd.read_parquet(directory_path + '/conditions.parquet/*.parquet')\n",
        "demographics_event_df = demographics_event_df.compute()\n",
        "demographics_event_df = demographics_event_df.reset_index(drop=True)\n",
        "save_path = '/content/drive/MyDrive/VCHAMPS - Train Cleaned-Mapped/conditions'\n",
        "process_and_save_chunks(demographics_event_df, save_path,df_name,'Age at event', 'Event date')"
      ],
      "metadata": {
        "id": "j_0uHsnlYNXw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}